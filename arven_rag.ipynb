{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alartuka/Arven/blob/main/arven_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAAj3apJmnAM"
      },
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter RAG Workshop\n",
        "\n",
        "**Skills: HuggingFace, LangChain, Pinecone**\n",
        "\n",
        "**Other Resources:**\n",
        "- [Get your Groq API Key](https://console.groq.com/keys)\n",
        "- [Get your Pinecone API Key](https://www.pinecone.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qof4EyLtm4-A"
      },
      "source": [
        "\n",
        "### What is RAG anyway?\n",
        "\n",
        "\n",
        "![withoutRAG](https://github.com/user-attachments/assets/649d6101-b63a-4750-997a-b6abc25e5609)\n",
        "\n",
        "![withRAG](https://github.com/user-attachments/assets/e6dd9c46-0bf9-4c31-bd72-a27939ef82b8)\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique primarily used in GenAI applications to improve the quality and accuracy of generated text by LLMs by combining two key processes: retrieval and generation.\n",
        "\n",
        "### Breaking It Down:\n",
        "#### Retrieval:\n",
        "\n",
        "- Before generating a response, the system first looks up relevant information from a large database or knowledge base. This is like searching through a library or the internet to find the most useful facts, articles, or data related to the question or topic.\n",
        "\n",
        "#### Generation:\n",
        "\n",
        "- Once the relevant information is retrieved, the system then uses it to help generate a response. This is where the model, like GPT, creates new text (answers, explanations, etc.) based on the retrieved information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKy5DKlZm0a6"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r5K71LmLS7ze",
        "outputId": "0333eb99-5b0f-4925-9eca-59e4ba6805ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Collecting groq\n",
            "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.11-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.18.11-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pdfminer==20191125\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfminer.six==20221105\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pillow_heif\n",
            "  Downloading pillow_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting unstructured_inference\n",
            "  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting exa-py\n",
            "  Downloading exa_py-1.14.18-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pycryptodome (from pdfminer==20191125)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20221105) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20221105) (43.0.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.71)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.7.14)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.5.0)\n",
            "Collecting pinecone<8.0.0,>=6.0.0 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain_pinecone)\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.41.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from pillow_heif) (11.3.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (0.33.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (4.12.0.88)\n",
            "Collecting onnx (from unstructured_inference)\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime>=1.18.0 (from unstructured_inference)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (2.6.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.0.18)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.16.0)\n",
            "Collecting pypdfium2 (from unstructured_inference)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.17.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured_inference) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured_inference) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured_inference) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured_inference) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.4.1)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_codspeed-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.18.0->unstructured_inference)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (1.13.1)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unstructured_inference)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.18.0->unstructured_inference) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured_inference) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured_inference) (2025.2)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->unstructured_inference) (0.21.0+cu124)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105) (2.22)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting packaging>=20.9 (from huggingface-hub->unstructured_inference)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.18.0->unstructured_inference)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured_inference) (3.0.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (9.0.0)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.1.2)\n",
            "Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading langchain_pinecone-0.2.11-py3-none-any.whl (23 kB)\n",
            "Downloading unstructured-0.18.11-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exa_py-1.14.18-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.41.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pdfminer, langdetect\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140080 sha256=a433f960e89469c2c23f459fc392691d8f7dfa08c40589adc94f226e77073d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/24/93/05316c6df89ff210a9a705060277e3acbfd2d1bd3a5853ee19\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=e73fa97cc2b16ce054bf70ef7299593315dfd20cd42659bcabc541b2f6463216\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built pdfminer langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, python-dotenv, pypdfium2, pypdf, pycryptodome, pinecone-plugin-interface, pillow_heif, packaging, onnx, olefile, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, langdetect, humanfriendly, httpx-sse, emoji, backoff, vcrpy, typing-inspect, python-oxmsg, pinecone-plugin-assistant, pinecone-client, pdfminer, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, coloredlogs, unstructured-client, syrupy, pytest-socket, pytest-recording, pytest-codspeed, pytest-benchmark, pytest-asyncio, pydantic-settings, pinecone, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, groq, dataclasses-json, aiohttp-retry, unstructured, exa-py, langchain-tests, langchain-openai, langchain_pinecone, unstructured_inference, langchain-community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain langchain-community openai groq tiktoken pinecone-client langchain_pinecone unstructured pdfminer==20191125 pdfminer.six==20221105 pillow_heif unstructured_inference sentence-transformers exa-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay8JWpWbWRQN"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, WebBaseLoader, YoutubeLoader, DirectoryLoader, TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import os\n",
        "from groq import Groq\n",
        "from exa_py import Exa\n",
        "import hashlib\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "openai_client = OpenAI()\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "exa_api_key = userdata.get(\"EXA_API_KEY\")\n",
        "os.environ['EXA_API_KEY'] = exa_api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgUhMx3EWjdH"
      },
      "source": [
        "# Initialize the HuggingFace Embeddings client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iziy1TmoWRSW"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFp3fV8YVaPX"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3UaJ_7FWn_h"
      },
      "outputs": [],
      "source": [
        "text = \"Hello my name is Faizan\"\n",
        "\n",
        "query_result = embeddings.embed_query(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI8p1gYw1A0O"
      },
      "outputs": [],
      "source": [
        "query_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F88E4puWoCH"
      },
      "outputs": [],
      "source": [
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVD9c7pdKvHi"
      },
      "source": [
        "# Initialize the Groq client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwXxemUuWqol"
      },
      "outputs": [],
      "source": [
        "# Free Llama 3.1 API via Groq\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "# initialize openAI client\n",
        "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN-sQbs7K3VS"
      },
      "source": [
        "# Calculating sentence similarity with embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrSQAyg7WqtF"
      },
      "outputs": [],
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)\n",
        "\n",
        "\n",
        "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
        "    # Get embeddings for both sentences\n",
        "    embedding1 = np.array(get_huggingface_embeddings(sentence1))\n",
        "    embedding2 = np.array(get_huggingface_embeddings(sentence2))\n",
        "\n",
        "    # Reshape embeddings for cosine_similarity function\n",
        "    embedding1 = embedding1.reshape(1, -1)\n",
        "    embedding2 = embedding2.reshape(1, -1)\n",
        "\n",
        "    print(\"Embedding for Sentence 1:\", embedding1)\n",
        "    print(\"\\nEmbedding for Sentence 2:\", embedding2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(embedding1, embedding2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"I like running to the park\"\n",
        "sentence2 = \"I like running to the office\"\n",
        "\n",
        "\n",
        "similarity = cosine_similarity_between_sentences(sentence1, sentence2)\n",
        "print(f\"\\n\\nCosine similarity between '{sentence1}' and '{sentence2}': {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r9IqANO9GGK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLSNcpnO9GI9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YA0Wj0X-QB2"
      },
      "source": [
        "# Load in the Data\n",
        "\n",
        "Learn more about the dataset [here](https://www.kaggle.com/datasets/ayoubcherguelaine/company-documents-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g33q9gobWqrE"
      },
      "outputs": [],
      "source": [
        "# ! kaggle datasets download -d ayoubcherguelaine/company-documents-dataset\n",
        "# ! unzip company-documents-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pVe0RMYXvIm"
      },
      "outputs": [],
      "source": [
        "# def process_directory(directory_path):\n",
        "#     data = []\n",
        "#     for root, _, files in os.walk(directory_path):\n",
        "#         for file in files:\n",
        "\n",
        "#             file_path = os.path.join(root, file)\n",
        "#             print(f\"Processing file: {file_path}\")\n",
        "#             loader = PyPDFLoader(file_path)\n",
        "#             data.append({\"File\": file_path, \"Data\": loader.load()})\n",
        "\n",
        "#     return data\n",
        "\n",
        "# directory_path = \"/content/CompanyDocuments\"\n",
        "# documents = process_directory(directory_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LmXXIb629Ir"
      },
      "outputs": [],
      "source": [
        "# documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8RG8D813Ljz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3vBKXSyUiib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhj2mq3fVEUY"
      },
      "source": [
        "# Initialize Exa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VavYEkoGVM9S"
      },
      "outputs": [],
      "source": [
        "exa = Exa(api_key = os.getenv('EXA_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wsFQpyXUbNp"
      },
      "source": [
        "Search and scrape Information/content about Aven from the web including the company's site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j0tJ_fW9GLB"
      },
      "outputs": [],
      "source": [
        "company_name = \"Aven\"\n",
        "num_results = 100\n",
        "all_results = []\n",
        "\n",
        "# Search queries for different types of content\n",
        "search_queries = [\n",
        "    f'site:aven.com aven card',\n",
        "    f'site:aven.com \"aven card\" support articles',\n",
        "    f'site:aven.com \"aven card\" education how it works',\n",
        "    f'site:aven.com \"aven card\" reviews and testimonials',\n",
        "    f'site:aven.com \"aven card\" about us'\n",
        "    # f'news articles blogs about \"{company_name}\" Card the financial services fintech company',\n",
        "    # f\"{company_name} financial services fintech\",\n",
        "    # f\"{company_name} company profile information about\",\n",
        "    # f\"{company_name} products services offerings\"\n",
        "]\n",
        "\n",
        "for query in search_queries:\n",
        "    try:\n",
        "        results = exa.search_and_contents(\n",
        "            query=query,\n",
        "            type=\"neural\",\n",
        "            num_results=num_results // len(search_queries),\n",
        "            text=True,\n",
        "            highlights=True,\n",
        "            use_autoprompt=True,\n",
        "            start_published_date=\"2023-01-01\"\n",
        "        )\n",
        "        all_results.extend(results.results)\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching for query '{query}': {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Total results collected: {len(all_results)}\")\n",
        "print(all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhnV95xyJs9x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC2OVunB-_gm"
      },
      "source": [
        "# Initialize Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saT1yD_mdHNa"
      },
      "outputs": [],
      "source": [
        "# Make sure to create a Pinecone index with 384 dimensions\n",
        "# You don't need to create a namespace through Pinecone, we will just define the name of the namespace here and use it later\n",
        "\n",
        "index_name = \"arven\"\n",
        "\n",
        "namespace = \"company-documents\"\n",
        "\n",
        "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6owEozIA--Tt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBwhuuLhAqG-"
      },
      "source": [
        "# Insert data into Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWRowUXGZJzj"
      },
      "outputs": [],
      "source": [
        "# Adapted document processing for Exa results\n",
        "# Complete fix with chunking and minimal metadata\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import hashlib\n",
        "\n",
        "# Initialize text splitter for chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Smaller chunks\n",
        "    chunk_overlap=200,    # Some overlap between chunks\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "document_data = []\n",
        "\n",
        "print(f\"Processing {len(all_results)} Exa results with chunking...\")\n",
        "\n",
        "for i, exa_result in enumerate(all_results):\n",
        "    try:\n",
        "        # Extract data from Exa result\n",
        "        document_content = getattr(exa_result, 'text', '') or getattr(exa_result, 'content', '')\n",
        "        document_source = getattr(exa_result, 'url', 'unknown')\n",
        "        title = getattr(exa_result, 'title', 'Unknown Title')\n",
        "        published_date = getattr(exa_result, 'published_date', '')\n",
        "\n",
        "        # Skip if no content\n",
        "        if not document_content or len(document_content.strip()) < 100:\n",
        "            continue\n",
        "\n",
        "        # Create unique content hash\n",
        "        content_hash = hashlib.md5(document_content.encode()).hexdigest()[:8]\n",
        "\n",
        "        # Split content into chunks\n",
        "        chunks = text_splitter.split_text(document_content)\n",
        "\n",
        "        # Process each chunk\n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            if len(chunk.strip()) < 50:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            # Create MINIMAL metadata (this is the key!)\n",
        "            minimal_metadata = {\n",
        "                \"source\": document_source[:200],  # Truncate URL if very long\n",
        "                \"title\": title[:100],             # Truncate title\n",
        "                \"chunk\": chunk_idx,               # Chunk index\n",
        "                \"doc_id\": f\"{content_hash}_{chunk_idx}\",  # Unique ID\n",
        "                \"company\": company_name,\n",
        "                \"date\": str(published_date)[:10] if published_date else \"\",  # Just date part\n",
        "            }\n",
        "\n",
        "            # Create simple page content (NO extra formatting that adds size)\n",
        "            page_content = chunk  # Just the chunk content, no extra wrapper\n",
        "\n",
        "            doc = Document(\n",
        "                page_content=page_content,\n",
        "                metadata=minimal_metadata\n",
        "            )\n",
        "\n",
        "            document_data.append(doc)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(all_results)} results...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing result {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Created {len(document_data)} document chunks\")\n",
        "\n",
        "# Test metadata size on a few samples\n",
        "import json\n",
        "\n",
        "def get_metadata_size(doc):\n",
        "    return len(json.dumps(doc.metadata).encode('utf-8'))\n",
        "\n",
        "print(\"\\n🔍 Checking metadata sizes:\")\n",
        "for i in range(min(5, len(document_data))):\n",
        "    size = get_metadata_size(document_data[i])\n",
        "    content_size = len(document_data[i].page_content)\n",
        "    print(f\"Doc {i+1}: metadata={size}B, content={content_size}B\")\n",
        "\n",
        "    if size > 40000:\n",
        "        print(f\"❌ STILL TOO LARGE: {size} bytes\")\n",
        "    else:\n",
        "        print(f\"✅ OK: {size} bytes\")\n",
        "\n",
        "# If you're STILL getting errors, try this ultra-minimal version:\n",
        "def create_ultra_minimal_docs(all_results):\n",
        "    \"\"\"Ultra minimal approach - absolutely bare minimum metadata\"\"\"\n",
        "    ultra_minimal_docs = []\n",
        "\n",
        "    for i, exa_result in enumerate(all_results):\n",
        "        try:\n",
        "            content = getattr(exa_result, 'text', '') or getattr(exa_result, 'content', '')\n",
        "            if not content or len(content.strip()) < 100:\n",
        "                continue\n",
        "\n",
        "            # Split into chunks\n",
        "            chunks = text_splitter.split_text(content)\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                if len(chunk.strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # ABSOLUTE MINIMUM metadata\n",
        "                doc = Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata={\n",
        "                        \"id\": f\"doc_{i}_{chunk_idx}\",\n",
        "                        \"company\": company_name\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                ultra_minimal_docs.append(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return ultra_minimal_docs\n",
        "\n",
        "# Uncomment this if you're still getting metadata errors:\n",
        "# print(\"Using ultra-minimal metadata approach...\")\n",
        "# document_data = create_ultra_minimal_docs(all_results)\n",
        "# print(f\"Created {len(document_data)} ultra-minimal documents\")\n",
        "\n",
        "print(f\"\\n✅ Ready to upload {len(document_data)} documents to Pinecone\")\n",
        "print(\"Now try running the PineconeVectorStore.from_documents() again\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQY6lExWaiOE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2D3BtRakHO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1drtS1Lcaodu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT1JvrNF--RM"
      },
      "outputs": [],
      "source": [
        "# for document in documents:\n",
        "#     print(document['File'], document['Data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81XIGJn0A2CU"
      },
      "outputs": [],
      "source": [
        "# Prepare the text for embedding\n",
        "# document_data = []\n",
        "# for document in documents:\n",
        "\n",
        "#     document_source = document['Data'][0].metadata['source']\n",
        "#     document_content = document['Data'][0].page_content\n",
        "\n",
        "#     file_name = document_source.split(\"/\")[-1]\n",
        "#     folder_names = document_source.split(\"/\")[2:-1]\n",
        "\n",
        "#     doc = Document(\n",
        "#         page_content = f\"<Source>\\n{document_source}\\n</Source>\\n\\n<Content>\\n{document_content}\\n</Content>\",\n",
        "#         metadata = {\n",
        "#             \"file_name\": file_name,\n",
        "#             \"parent_folder\": folder_names[-1],\n",
        "#             \"folder_names\": folder_names\n",
        "#         }\n",
        "#     )\n",
        "#     document_data.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMuoe2rZbIln"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Hb-KrfY-BdCv"
      },
      "outputs": [],
      "source": [
        "document_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LPcxIMBBFqG"
      },
      "outputs": [],
      "source": [
        "# Insert documents into Pinecone\n",
        "vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
        "    document_data,\n",
        "    embeddings,\n",
        "    index_name=index_name,\n",
        "    namespace=namespace\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwIix4L7A2Ez"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PED8FwrW--O0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uCoLtHtLwlI"
      },
      "source": [
        "# Perform RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMk_-E0EI0SE"
      },
      "outputs": [],
      "source": [
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_queries = [\n",
        "    # Product/Service Questions\n",
        "    \"What does Aven do?\",\n",
        "    \"What services does Aven offer?\",\n",
        "    \"How does Aven's credit card work?\",\n",
        "    \"What are the benefits of using Aven?\",\n",
        "    \"What makes Aven different from other fintech companies?\",\n",
        "\n",
        "    # Eligibility/Requirements\n",
        "    \"What credit score do I need for Aven?\",\n",
        "    \"Who is eligible for Aven's services?\",\n",
        "    \"How do I apply for an Aven card?\",\n",
        "    \"What documents do I need to apply?\",\n",
        "    \"Can I get approved with bad credit?\",\n",
        "\n",
        "    # Fees/Pricing\n",
        "    \"What are Aven's fees?\",\n",
        "    \"Does Aven charge annual fees?\",\n",
        "    \"What are the interest rates?\",\n",
        "    \"Are there any hidden costs?\",\n",
        "    \"How much does it cost to use Aven?\",\n",
        "\n",
        "    # Security/Trust\n",
        "    \"Is Aven FDIC insured?\",\n",
        "    \"How secure is Aven?\",\n",
        "    \"Is my money safe with Aven?\",\n",
        "    \"What security measures does Aven have?\",\n",
        "    \"Is Aven regulated?\",\n",
        "\n",
        "    # Comparison/Competition\n",
        "    \"Who are Aven's competitors?\",\n",
        "    \"How does Aven compare to traditional banks?\",\n",
        "    \"Is Aven better than Chime?\",\n",
        "    \"What's the difference between Aven and other fintech companies?\",\n",
        "    \"Why should I choose Aven over other options?\",\n",
        "\n",
        "    # Features/Functionality\n",
        "    \"What features does Aven offer?\",\n",
        "    \"Can I use Aven internationally?\",\n",
        "    \"Does Aven have a mobile app?\",\n",
        "    \"What payment methods does Aven accept?\",\n",
        "    \"Can I set up direct deposit with Aven?\",\n",
        "\n",
        "    # Business/Company Info\n",
        "    \"When was Aven founded?\",\n",
        "    \"Who founded Aven?\",\n",
        "    \"Where is Aven headquartered?\",\n",
        "    \"How much funding has Aven raised?\",\n",
        "    \"What's Aven's business model?\",\n",
        "\n",
        "    # Customer Support\n",
        "    \"How do I contact Aven customer support?\",\n",
        "    \"What are Aven's support hours?\",\n",
        "    \"How do I report a problem with my Aven account?\",\n",
        "    \"Does Aven have phone support?\",\n",
        "    \"How long does Aven take to respond to support requests?\",\n",
        "\n",
        "    # Account Management\n",
        "    \"How do I close my Aven account?\",\n",
        "    \"Can I change my Aven account settings?\",\n",
        "    \"How do I update my information with Aven?\",\n",
        "    \"What happens if I miss a payment?\",\n",
        "    \"How do I increase my credit limit?\",\n",
        "\n",
        "    # Recent/News\n",
        "    \"What's new with Aven?\",\n",
        "    \"Has Aven launched any new products recently?\",\n",
        "    \"What are people saying about Aven?\",\n",
        "    \"Any recent news about Aven?\",\n",
        "    \"What's Aven's latest update?\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ro-0Qh7P6c6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQCna1vZI0Un"
      },
      "outputs": [],
      "source": [
        "# query = \"What are some items that Pirkko Koskitalo is likely to buy next? What incentives can I put in place to ensure he orders more?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "query = random.choice(sample_queries)\n",
        "\n",
        "print(query)"
      ],
      "metadata": {
        "id": "XzH2uVrX02vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgpTn-j-MLAN"
      },
      "outputs": [],
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyU4z3JEMLEb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "raw_query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCL3shOvOTZg"
      },
      "outputs": [],
      "source": [
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=3, include_metadata=True, namespace=namespace)\n",
        "# top_k=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b613bS_gMLG5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "top_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_IO4A1bPRri"
      },
      "outputs": [],
      "source": [
        "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5ndG43QPVjR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-udZviyaPVlz"
      },
      "outputs": [],
      "source": [
        "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C9IRldxPRt4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hTsPfASy9UQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "Your name is Arven.\n",
        "\n",
        "You are the official AI assistant for Aven, a financial services and fintech company. Your role is to provide helpful, accurate, and trustworthy information about Aven's products, services, and company to customers and prospective customers.\n",
        "\n",
        "Core Guidelines:\n",
        "\n",
        "1. Answer Based on Provided Data\n",
        "- ONLY answer questions using information from the context data provided to you\n",
        "- If the provided context doesn't contain sufficient information to answer a question, clearly state: \"I don't have enough information in my current knowledge base to answer that question accurately. For the most up-to-date information, please contact Aven customer support or visit our website.\"\n",
        "- Never make up or infer information that isn't explicitly stated in the provided context\n",
        "\n",
        "2. Tone and Voice\n",
        "- Professional yet approachable: Sound knowledgeable but not overly technical\n",
        "- Helpful and customer-focused: Prioritize solving the user's problem or answering their question\n",
        "- Trustworthy: Be transparent about limitations and always provide accurate information\n",
        "- Concise but complete: Give thorough answers without being unnecessarily verbose\n",
        "\n",
        "3. Response Structure\n",
        "For each response:\n",
        "1. Start with a clear, direct answer to the user's question\n",
        "2. Provide relevant context and details from the provided data\n",
        "3. Suggest relevant actions the user can take (when appropriate)\n",
        "\n",
        "4. Specific Topics to Handle\n",
        "\n",
        "Products & Services:\n",
        "- Explain Aven's financial products and services clearly\n",
        "- Highlight key features and benefits\n",
        "- Compare different options when relevant\n",
        "\n",
        "Eligibility & Applications:\n",
        "- Provide clear information about requirements\n",
        "- Guide users through application processes\n",
        "- Be transparent about approval criteria\n",
        "\n",
        "Fees & Pricing:\n",
        "- Give accurate, up-to-date fee information\n",
        "- Explain any conditions or variables\n",
        "- Be transparent about all costs\n",
        "\n",
        "Security & Trust:\n",
        "- Emphasize Aven's security measures and regulatory compliance\n",
        "- Address customer concerns about safety and legitimacy\n",
        "- Provide reassurance backed by facts\n",
        "\n",
        "Customer Support:\n",
        "- Direct users to appropriate support channels\n",
        "- Provide contact information and hours\n",
        "- Set clear expectations for response times\n",
        "\n",
        "5. What NOT to Do\n",
        "- Don't provide financial advice or recommendations beyond explaining Aven's products\n",
        "- Don't make promises about approval, rates, or terms that aren't guaranteed\n",
        "- Don't share sensitive customer information or ask for personal details\n",
        "- Don't speculate about future products or company plans not mentioned in your data\n",
        "- Don't disparage competitors - focus on Aven's strengths\n",
        "- Don't provide information about other companies unless it's in your provided context\n",
        "\n",
        "6. When You Don't Know\n",
        "If you encounter questions about:\n",
        "- Specific account issues: Direct to customer support\n",
        "- Information not in your data: Clearly state limitations and suggest official channels\n",
        "- Technical problems: Provide troubleshooting steps if available, otherwise direct to support\n",
        "- Legal or regulatory questions: Direct to official documentation or support\n",
        "\n",
        "Example Response Format\n",
        "\n",
        "User Question: \"What credit score do I need for an Aven card?\"\n",
        "\n",
        "Good Response:\n",
        "\"Aven typically considers applicants with credit scores of [specific range if provided in context]. However, credit score is just one factor in our approval process - we also consider [other factors mentioned in context].\n",
        "\n",
        "Aven focuses on [unique approach mentioned in data]. This means that even if your credit score is [relevant details from context].\n",
        "\n",
        "To apply and get a personalized assessment, you can [application process from context]. For specific questions about your eligibility, I recommend contacting Aven customer support at [contact info from context].\"\n",
        "\n",
        "Remember:\n",
        "- You represent Aven - be professional and helpful\n",
        "- Accuracy is more important than being comprehensive\n",
        "- When in doubt, direct users to official channels\n",
        "- Always prioritize the customer's needs and experience\n",
        "- Use only the information provided in your knowledge base\n",
        "\n",
        "Your goal is to be the most helpful, accurate, and trustworthy source of information about Aven while staying within the bounds of your provided data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O4mXn2Eb9_ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WdCT44fQAty"
      },
      "outputs": [],
      "source": [
        "# system_prompt = f\"\"\"You are an expert at understanding and analyzing company data - particularly shipping orders, purchase orders, invoices, and inventory reports.\n",
        "\n",
        "# Answer any questions I have, based on the data provided. Always consider all of the context provided when forming a response.\n",
        "# \"\"\"\n",
        "\n",
        "llm_response = groq_client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kezebsTcPrs3"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tTAtCQzUAyZ"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ2Y6R5DPru0"
      },
      "outputs": [],
      "source": [
        "def perform_rag(query):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    query_embedding = np.array(raw_query_embedding)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    system_prompt = f\"\"\"Your name is Arven.\n",
        "\n",
        "You are the official AI Customer Support agent for Aven, a financial services and fintech company. Your role is to provide helpful, accurate, and trustworthy information about Aven's products, services, and company to customers and prospective customers.\n",
        "\n",
        "Core Guidelines:\n",
        "\n",
        "1. Answer Based on Provided Data\n",
        "- ONLY answer questions using information from the context data provided to you\n",
        "- If the provided context doesn't contain sufficient information to answer a question, clearly state: \"I don't have enough information in my current knowledge base to answer that question accurately. For the most up-to-date information, please contact Aven customer support or visit our website.\"\n",
        "- Never make up or infer information that isn't explicitly stated in the provided context\n",
        "\n",
        "2. Tone and Voice\n",
        "- Professional yet approachable: Sound knowledgeable but not overly technical\n",
        "- Helpful and customer-focused: Prioritize solving the user's problem or answering their question\n",
        "- Trustworthy: Be transparent about limitations and always provide accurate information\n",
        "- Concise but complete: Give thorough answers without being unnecessarily verbose\n",
        "\n",
        "3. Response Structure\n",
        "For each response:\n",
        "1. Start with a clear, direct answer to the user's question\n",
        "2. Provide relevant context and details from the provided data\n",
        "3. Suggest relevant actions the user can take (when appropriate)\n",
        "\n",
        "4. Specific Topics to Handle\n",
        "\n",
        "Products & Services:\n",
        "- Explain Aven's financial products and services clearly\n",
        "- Highlight key features and benefits\n",
        "- Compare different options when relevant\n",
        "\n",
        "Eligibility & Applications:\n",
        "- Provide clear information about requirements\n",
        "- Guide users through application processes\n",
        "- Be transparent about approval criteria\n",
        "\n",
        "Fees & Pricing:\n",
        "- Give accurate, up-to-date fee information\n",
        "- Explain any conditions or variables\n",
        "- Be transparent about all costs\n",
        "\n",
        "Security & Trust:\n",
        "- Emphasize Aven's security measures and regulatory compliance\n",
        "- Address customer concerns about safety and legitimacy\n",
        "- Provide reassurance backed by facts\n",
        "\n",
        "Customer Support:\n",
        "- Direct users to appropriate support channels\n",
        "- Provide contact information and hours\n",
        "- Set clear expectations for response times\n",
        "\n",
        "5. What NOT to Do\n",
        "- Don't provide financial advice or recommendations beyond explaining Aven's products\n",
        "- Don't make promises about approval, rates, or terms that aren't guaranteed\n",
        "- Don't share sensitive customer information or ask for personal details\n",
        "- Don't speculate about future products or company plans not mentioned in your data\n",
        "- Don't disparage competitors - focus on Aven's strengths\n",
        "- Don't provide information about other companies unless it's in your provided context\n",
        "\n",
        "6. When You Don't Know\n",
        "If you encounter questions about:\n",
        "- Specific account issues: Direct to customer support\n",
        "- Information not in your data: Clearly state limitations and suggest official channels\n",
        "- Technical problems: Provide troubleshooting steps if available, otherwise direct to support\n",
        "- Legal or regulatory questions: Direct to official documentation or support\n",
        "\n",
        "Example Response Format\n",
        "\n",
        "User Question: \"What credit score do I need for an Aven card?\"\n",
        "\n",
        "Good Response:\n",
        "\"Aven typically considers applicants with credit scores of [specific range if provided in context]. However, credit score is just one factor in our approval process - we also consider [other factors mentioned in context].\n",
        "\n",
        "Aven focuses on [unique approach mentioned in data]. This means that even if your credit score is [relevant details from context].\n",
        "\n",
        "To apply and get a personalized assessment, you can [application process from context]. For specific questions about your eligibility, I recommend contacting Aven customer support at [contact info from context].\"\n",
        "\n",
        "Remember:\n",
        "- You represent Aven - be professional and helpful\n",
        "- Accuracy is more important than being comprehensive\n",
        "- When in doubt, direct users to official channels\n",
        "- Always prioritize the customer's needs and experience\n",
        "- Use only the information provided in your knowledge base\n",
        "\n",
        "Your goal is to be the most helpful, accurate, and trustworthy source of information about Aven while staying within the bounds of your provided data.\n",
        "\"\"\"\n",
        "\n",
        "    res = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.1-70b-versatile\", # llama-3.1-70b-versatile\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return res.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFqRJjKYLB13"
      },
      "outputs": [],
      "source": [
        "# If you have access to the o1 model through the OpenAI API, you can use this function to compare the quality of responses\n",
        "def perform_rag_openai(query):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    query_embedding = np.array(raw_query_embedding)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    # Modify the prompt below as need to improve the response quality\n",
        "    system_prompt = f\"\"\"You are an expert at understanding and analyzing company data - particularly shipping orders, purchase orders, invoices, and inventory reports.\n",
        "\n",
        "    Answer any questions I have, based on the data provided. Always consider all parts of the context provided when forming a response.\n",
        "    \"\"\"\n",
        "\n",
        "    res = openai_client.chat.completions.create(\n",
        "        model=\"o1-preview\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": f\"{system_prompt} {augmented_query}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return res.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSests6uWRU1"
      },
      "outputs": [],
      "source": [
        "response = perform_rag(\"What are some trends with Ricardo Adocicados purchase orders?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtXn9S5TLIO3"
      },
      "outputs": [],
      "source": [
        "response = perform_rag_openai(\"What are some trends with Ricardo Adocicados purchase orders?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzfIFuNCUlvk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHjyXFqVfne"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoKC4-DXVfp4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk49DaWtTZem"
      },
      "outputs": [],
      "source": [
        "vectors_to_upsert = []\n",
        "processed_count = 0\n",
        "\n",
        "print(f\"📊 Processing {len(all_results)} Exa results...\")\n",
        "\n",
        "for result in all_results:\n",
        "    try:\n",
        "        # Extract content from Exa result\n",
        "        content = getattr(result, 'text', '') or getattr(result, 'content', '')\n",
        "        url = getattr(result, 'url', 'unknown')\n",
        "        title = getattr(result, 'title', 'Unknown Title')\n",
        "        # We won't use published_date, content_hash, scraped_at, source in minimal metadata\n",
        "\n",
        "        if not content or len(content) < 100:\n",
        "            continue\n",
        "\n",
        "        # No need for content_hash if we are using minimal metadata\n",
        "\n",
        "        # Chunk the content\n",
        "        chunk_size = 1000\n",
        "        overlap = 200\n",
        "\n",
        "        if len(content) < chunk_size:\n",
        "            chunks = [content]\n",
        "        else:\n",
        "            chunks = []\n",
        "            start = 0\n",
        "\n",
        "            while start < len(content):\n",
        "                end = start + chunk_size\n",
        "\n",
        "                # Try to break at sentence boundary\n",
        "                if end < len(content):\n",
        "                    last_period = content.rfind('.', start, end)\n",
        "                    if last_period > start + chunk_size // 2:\n",
        "                        end = last_period + 1\n",
        "\n",
        "                chunk = content[start:end].strip()\n",
        "                if chunk and len(chunk) > 50:\n",
        "                    chunks.append(chunk)\n",
        "\n",
        "                # Move start position with overlap\n",
        "                start = end - overlap if end < len(content) else end\n",
        "\n",
        "        # Process each chunk\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if len(chunk.strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            # Generate embedding\n",
        "            embedding = embeddings.embed_query(chunk).tolist()\n",
        "\n",
        "            # Create unique ID (using a combination of URL and chunk index)\n",
        "            # Using a hash of the URL to keep ID length reasonable\n",
        "            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n",
        "            chunk_id = f\"{url_hash}_{i}\"\n",
        "\n",
        "\n",
        "            # Prepare MINIMAL metadata to ensure it's within limits\n",
        "            metadata = {\n",
        "                'source_url': url,\n",
        "                'title': title[:200], # Truncate title to a reasonable length\n",
        "                'chunk_index': i,\n",
        "            }\n",
        "\n",
        "            vectors_to_upsert.append({\n",
        "                'id': chunk_id,\n",
        "                'values': embedding,\n",
        "                'metadata': metadata\n",
        "            })\n",
        "\n",
        "        processed_count += 1\n",
        "        print(f\"✅ Processed {processed_count}/{len(all_results)}: {title[:50]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing result: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nFinished processing. Prepared {len(vectors_to_upsert)} vectors for upsert.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UFp3fV8YVaPX",
        "7YA0Wj0X-QB2"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}