{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alartuka/Arven/blob/main/src/arven_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAAj3apJmnAM"
      },
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter RAG Workshop\n",
        "\n",
        "**Skills: HuggingFace, LangChain, Pinecone**\n",
        "\n",
        "**Other Resources:**\n",
        "- [Get your Groq API Key](https://console.groq.com/keys)\n",
        "- [Get your Pinecone API Key](https://www.pinecone.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qof4EyLtm4-A"
      },
      "source": [
        "\n",
        "### What is RAG anyway?\n",
        "\n",
        "\n",
        "![withoutRAG](https://github.com/user-attachments/assets/649d6101-b63a-4750-997a-b6abc25e5609)\n",
        "\n",
        "![withRAG](https://github.com/user-attachments/assets/e6dd9c46-0bf9-4c31-bd72-a27939ef82b8)\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique primarily used in GenAI applications to improve the quality and accuracy of generated text by LLMs by combining two key processes: retrieval and generation.\n",
        "\n",
        "### Breaking It Down:\n",
        "#### Retrieval:\n",
        "\n",
        "- Before generating a response, the system first looks up relevant information from a large database or knowledge base. This is like searching through a library or the internet to find the most useful facts, articles, or data related to the question or topic.\n",
        "\n",
        "#### Generation:\n",
        "\n",
        "- Once the relevant information is retrieved, the system then uses it to help generate a response. This is where the model, like GPT, creates new text (answers, explanations, etc.) based on the retrieved information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKy5DKlZm0a6"
      },
      "source": [
        "# Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r5K71LmLS7ze"
      },
      "outputs": [],
      "source": [
        "! pip install langchain langchain-community openai groq tiktoken pinecone-client langchain_pinecone unstructured pdfminer==20191125 pdfminer.six==20221105 pillow_heif unstructured_inference sentence-transformers exa-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay8JWpWbWRQN"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, WebBaseLoader, YoutubeLoader, DirectoryLoader, TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import os\n",
        "from groq import Groq\n",
        "from exa_py import Exa\n",
        "import hashlib\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "# os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "# openai_client = OpenAI()\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "exa_api_key = userdata.get(\"EXA_API_KEY\")\n",
        "os.environ['EXA_API_KEY'] = exa_api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgUhMx3EWjdH"
      },
      "source": [
        "# Initialize the HuggingFace Embeddings client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iziy1TmoWRSW"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFp3fV8YVaPX"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3UaJ_7FWn_h"
      },
      "outputs": [],
      "source": [
        "# text = \"Hello my name is Faizan\"\n",
        "\n",
        "# query_result = embeddings.embed_query(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI8p1gYw1A0O"
      },
      "outputs": [],
      "source": [
        "# query_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F88E4puWoCH"
      },
      "outputs": [],
      "source": [
        "# len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVD9c7pdKvHi"
      },
      "source": [
        "# Initialize the Groq client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwXxemUuWqol"
      },
      "outputs": [],
      "source": [
        "# Free Llama 3.1 API via Groq\n",
        "\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "# initialize openAI client\n",
        "# openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN-sQbs7K3VS"
      },
      "source": [
        "# Calculating sentence similarity with embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrSQAyg7WqtF"
      },
      "outputs": [],
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)\n",
        "\n",
        "\n",
        "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
        "    # Get embeddings for both sentences\n",
        "    embedding1 = np.array(get_huggingface_embeddings(sentence1))\n",
        "    embedding2 = np.array(get_huggingface_embeddings(sentence2))\n",
        "\n",
        "    # Reshape embeddings for cosine_similarity function\n",
        "    embedding1 = embedding1.reshape(1, -1)\n",
        "    embedding2 = embedding2.reshape(1, -1)\n",
        "\n",
        "    print(\"Embedding for Sentence 1:\", embedding1)\n",
        "    print(\"\\nEmbedding for Sentence 2:\", embedding2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(embedding1, embedding2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"I like running to the park\"\n",
        "sentence2 = \"I like running to the office\"\n",
        "\n",
        "\n",
        "similarity = cosine_similarity_between_sentences(sentence1, sentence2)\n",
        "print(f\"\\n\\nCosine similarity between '{sentence1}' and '{sentence2}': {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r9IqANO9GGK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLSNcpnO9GI9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YA0Wj0X-QB2"
      },
      "source": [
        "# Load in the Data\n",
        "\n",
        "Learn more about the dataset [here](https://www.kaggle.com/datasets/ayoubcherguelaine/company-documents-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g33q9gobWqrE"
      },
      "outputs": [],
      "source": [
        "# ! kaggle datasets download -d ayoubcherguelaine/company-documents-dataset\n",
        "# ! unzip company-documents-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pVe0RMYXvIm"
      },
      "outputs": [],
      "source": [
        "# def process_directory(directory_path):\n",
        "#     data = []\n",
        "#     for root, _, files in os.walk(directory_path):\n",
        "#         for file in files:\n",
        "\n",
        "#             file_path = os.path.join(root, file)\n",
        "#             print(f\"Processing file: {file_path}\")\n",
        "#             loader = PyPDFLoader(file_path)\n",
        "#             data.append({\"File\": file_path, \"Data\": loader.load()})\n",
        "\n",
        "#     return data\n",
        "\n",
        "# directory_path = \"/content/CompanyDocuments\"\n",
        "# documents = process_directory(directory_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LmXXIb629Ir"
      },
      "outputs": [],
      "source": [
        "# documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8RG8D813Ljz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3vBKXSyUiib"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhj2mq3fVEUY"
      },
      "source": [
        "# Initialize Exa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VavYEkoGVM9S"
      },
      "outputs": [],
      "source": [
        "exa = Exa(api_key = os.getenv('EXA_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wsFQpyXUbNp"
      },
      "source": [
        "Search and scrape Information/content about Aven from the web including the company's site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j0tJ_fW9GLB"
      },
      "outputs": [],
      "source": [
        "company_name = \"Aven\"\n",
        "num_results = 100\n",
        "# all_results = []\n",
        "\n",
        "# Search queries for different types of content\n",
        "# search_queries = [\n",
        "#     f'site:aven.com aven card',\n",
        "#     f'site:aven.com \"aven card\" support articles',\n",
        "#     f'site:aven.com \"aven card\" education how it works',\n",
        "#     f'site:aven.com \"aven card\" reviews and testimonials',\n",
        "#     f'site:aven.com \"aven card\" about us'\n",
        "#     # f'news articles blogs about \"{company_name}\" Card the financial services fintech company',\n",
        "#     # f\"{company_name} financial services fintech\",\n",
        "#     # f\"{company_name} company profile information about\",\n",
        "#     # f\"{company_name} products services offerings\"\n",
        "# ]\n",
        "\n",
        "# for query in search_queries:\n",
        "#     try:\n",
        "#         results = exa.search_and_contents(\n",
        "#             query=query,\n",
        "#             type=\"neural\",\n",
        "#             num_results=num_results // len(search_queries),\n",
        "#             text=True,\n",
        "#             highlights=True,\n",
        "#             use_autoprompt=True,\n",
        "#             start_published_date=\"2023-01-01\"\n",
        "#         )\n",
        "#         all_results.extend(results.results)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error searching for query '{query}': {e}\")\n",
        "#         continue\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Total results collected: {len(all_results)}\")\n",
        "# print(all_results)\n",
        "\n",
        "response = exa.get_contents(\n",
        "    [\"https://aven.com\"],  # Full URL required\n",
        "    text=True,             # Get the full text\n",
        "    subpages=20,           # Crawl up to 20 subpages\n",
        "    subpage_target=[\"about\", \"faq\", \"pricing\", \"support\", \"education\", \"offer\", \"reviews\"]\n",
        ")\n",
        "all_results = response.results\n",
        "\n",
        "print(f\"Total results collected: {len(all_results)}\")\n",
        "print(all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhnV95xyJs9x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC2OVunB-_gm"
      },
      "source": [
        "# Initialize Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saT1yD_mdHNa"
      },
      "outputs": [],
      "source": [
        "# Make sure to create a Pinecone index with 384 dimensions\n",
        "# You don't need to create a namespace through Pinecone, we will just define the name of the namespace here and use it later\n",
        "\n",
        "index_name = \"arven\"\n",
        "\n",
        "namespace = \"company-documents\"\n",
        "\n",
        "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6owEozIA--Tt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBwhuuLhAqG-"
      },
      "source": [
        "# Insert data into Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWRowUXGZJzj"
      },
      "outputs": [],
      "source": [
        "# Adapted document processing for Exa results\n",
        "# Complete fix with chunking and minimal metadata\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import hashlib\n",
        "\n",
        "# Initialize text splitter for chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Smaller chunks\n",
        "    chunk_overlap=200,    # Some overlap between chunks\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "document_data = []\n",
        "\n",
        "print(f\"Processing {len(all_results)} Exa results with chunking...\")\n",
        "\n",
        "for i, exa_result in enumerate(all_results):\n",
        "    try:\n",
        "        # Extract data from Exa result\n",
        "        document_content = getattr(exa_result, 'text', '') or getattr(exa_result, 'content', '')\n",
        "        document_source = getattr(exa_result, 'url', 'unknown')\n",
        "        title = getattr(exa_result, 'title', 'Unknown Title')\n",
        "        published_date = getattr(exa_result, 'published_date', '')\n",
        "\n",
        "        # Skip if no content\n",
        "        if not document_content or len(document_content.strip()) < 100:\n",
        "            continue\n",
        "\n",
        "        # Create unique content hash\n",
        "        content_hash = hashlib.md5(document_content.encode()).hexdigest()[:8]\n",
        "\n",
        "        # Split content into chunks\n",
        "        chunks = text_splitter.split_text(document_content)\n",
        "\n",
        "        # Process each chunk\n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            if len(chunk.strip()) < 50:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            # Create MINIMAL metadata (this is the key!)\n",
        "            minimal_metadata = {\n",
        "                \"source\": document_source[:200],  # Truncate URL if very long\n",
        "                \"title\": title[:100],             # Truncate title\n",
        "                \"chunk\": chunk_idx,               # Chunk index\n",
        "                \"doc_id\": f\"{content_hash}_{chunk_idx}\",  # Unique ID\n",
        "                \"company\": company_name,\n",
        "                \"date\": str(published_date)[:10] if published_date else \"\",  # Just date part\n",
        "            }\n",
        "\n",
        "            # Create simple page content (NO extra formatting that adds size)\n",
        "            page_content = chunk  # Just the chunk content, no extra wrapper\n",
        "\n",
        "            doc = Document(\n",
        "                page_content=page_content,\n",
        "                metadata=minimal_metadata\n",
        "            )\n",
        "\n",
        "            document_data.append(doc)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(all_results)} results...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing result {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Created {len(document_data)} document chunks\")\n",
        "\n",
        "# Test metadata size on a few samples\n",
        "import json\n",
        "\n",
        "def get_metadata_size(doc):\n",
        "    return len(json.dumps(doc.metadata).encode('utf-8'))\n",
        "\n",
        "print(\"\\n🔍 Checking metadata sizes:\")\n",
        "for i in range(min(5, len(document_data))):\n",
        "    size = get_metadata_size(document_data[i])\n",
        "    content_size = len(document_data[i].page_content)\n",
        "    print(f\"Doc {i+1}: metadata={size}B, content={content_size}B\")\n",
        "\n",
        "    if size > 40000:\n",
        "        print(f\"❌ STILL TOO LARGE: {size} bytes\")\n",
        "    else:\n",
        "        print(f\"✅ OK: {size} bytes\")\n",
        "\n",
        "# If you're STILL getting errors, try this ultra-minimal version:\n",
        "def create_ultra_minimal_docs(all_results):\n",
        "    \"\"\"Ultra minimal approach - absolutely bare minimum metadata\"\"\"\n",
        "    ultra_minimal_docs = []\n",
        "\n",
        "    for i, exa_result in enumerate(all_results):\n",
        "        try:\n",
        "            content = getattr(exa_result, 'text', '') or getattr(exa_result, 'content', '')\n",
        "            if not content or len(content.strip()) < 100:\n",
        "                continue\n",
        "\n",
        "            # Split into chunks\n",
        "            chunks = text_splitter.split_text(content)\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                if len(chunk.strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # ABSOLUTE MINIMUM metadata\n",
        "                doc = Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata={\n",
        "                        \"id\": f\"doc_{i}_{chunk_idx}\",\n",
        "                        \"company\": company_name\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                ultra_minimal_docs.append(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return ultra_minimal_docs\n",
        "\n",
        "# Uncomment this if you're still getting metadata errors:\n",
        "# print(\"Using ultra-minimal metadata approach...\")\n",
        "# document_data = create_ultra_minimal_docs(all_results)\n",
        "# print(f\"Created {len(document_data)} ultra-minimal documents\")\n",
        "\n",
        "print(f\"\\n✅ Ready to upload {len(document_data)} documents to Pinecone\")\n",
        "print(\"Now try running the PineconeVectorStore.from_documents() again\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQY6lExWaiOE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb2D3BtRakHO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1drtS1Lcaodu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT1JvrNF--RM"
      },
      "outputs": [],
      "source": [
        "# for document in documents:\n",
        "#     print(document['File'], document['Data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81XIGJn0A2CU"
      },
      "outputs": [],
      "source": [
        "# Prepare the text for embedding\n",
        "# document_data = []\n",
        "# for document in documents:\n",
        "\n",
        "#     document_source = document['Data'][0].metadata['source']\n",
        "#     document_content = document['Data'][0].page_content\n",
        "\n",
        "#     file_name = document_source.split(\"/\")[-1]\n",
        "#     folder_names = document_source.split(\"/\")[2:-1]\n",
        "\n",
        "#     doc = Document(\n",
        "#         page_content = f\"<Source>\\n{document_source}\\n</Source>\\n\\n<Content>\\n{document_content}\\n</Content>\",\n",
        "#         metadata = {\n",
        "#             \"file_name\": file_name,\n",
        "#             \"parent_folder\": folder_names[-1],\n",
        "#             \"folder_names\": folder_names\n",
        "#         }\n",
        "#     )\n",
        "#     document_data.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMuoe2rZbIln"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Hb-KrfY-BdCv"
      },
      "outputs": [],
      "source": [
        "document_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LPcxIMBBFqG"
      },
      "outputs": [],
      "source": [
        "# Insert documents into Pinecone\n",
        "vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
        "    document_data,\n",
        "    embeddings,\n",
        "    index_name=index_name,\n",
        "    namespace=namespace\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwIix4L7A2Ez"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PED8FwrW--O0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uCoLtHtLwlI"
      },
      "source": [
        "# Perform RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMk_-E0EI0SE"
      },
      "outputs": [],
      "source": [
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_queries = [\n",
        "    # Product/Service Questions\n",
        "    \"What does Aven do?\",\n",
        "    \"What services does Aven offer?\",\n",
        "    \"How does Aven's credit card work?\",\n",
        "    \"What are the benefits of using Aven?\",\n",
        "    \"What makes Aven different from other fintech companies?\",\n",
        "\n",
        "    # Eligibility/Requirements\n",
        "    \"What credit score do I need for Aven?\",\n",
        "    \"Who is eligible for Aven's services?\",\n",
        "    \"How do I apply for an Aven card?\",\n",
        "    \"What documents do I need to apply?\",\n",
        "    \"Can I get approved with bad credit?\",\n",
        "\n",
        "    # Fees/Pricing\n",
        "    \"What are Aven's fees?\",\n",
        "    \"Does Aven charge annual fees?\",\n",
        "    \"What are the interest rates?\",\n",
        "    \"Are there any hidden costs?\",\n",
        "    \"How much does it cost to use Aven?\",\n",
        "\n",
        "    # Security/Trust\n",
        "    \"Is Aven FDIC insured?\",\n",
        "    \"How secure is Aven?\",\n",
        "    \"Is my money safe with Aven?\",\n",
        "    \"What security measures does Aven have?\",\n",
        "    \"Is Aven regulated?\",\n",
        "\n",
        "    # Comparison/Competition\n",
        "    \"Who are Aven's competitors?\",\n",
        "    \"How does Aven compare to traditional banks?\",\n",
        "    \"Is Aven better than Chime?\",\n",
        "    \"What's the difference between Aven and other fintech companies?\",\n",
        "    \"Why should I choose Aven over other options?\",\n",
        "\n",
        "    # Features/Functionality\n",
        "    \"What features does Aven offer?\",\n",
        "    \"Can I use Aven internationally?\",\n",
        "    \"Does Aven have a mobile app?\",\n",
        "    \"What payment methods does Aven accept?\",\n",
        "    \"Can I set up direct deposit with Aven?\",\n",
        "\n",
        "    # Business/Company Info\n",
        "    \"When was Aven founded?\",\n",
        "    \"Who founded Aven?\",\n",
        "    \"Where is Aven headquartered?\",\n",
        "    \"How much funding has Aven raised?\",\n",
        "    \"What's Aven's business model?\",\n",
        "\n",
        "    # Customer Support\n",
        "    \"How do I contact Aven customer support?\",\n",
        "    \"What are Aven's support hours?\",\n",
        "    \"How do I report a problem with my Aven account?\",\n",
        "    \"Does Aven have phone support?\",\n",
        "    \"How long does Aven take to respond to support requests?\",\n",
        "\n",
        "    # Account Management\n",
        "    \"How do I close my Aven account?\",\n",
        "    \"Can I change my Aven account settings?\",\n",
        "    \"How do I update my information with Aven?\",\n",
        "    \"What happens if I miss a payment?\",\n",
        "    \"How do I increase my credit limit?\",\n",
        "\n",
        "    # Recent/News\n",
        "    \"What's new with Aven?\",\n",
        "    \"Has Aven launched any new products recently?\",\n",
        "    \"What are people saying about Aven?\",\n",
        "    \"Any recent news about Aven?\",\n",
        "    \"What's Aven's latest update?\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ro-0Qh7P6c6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQCna1vZI0Un"
      },
      "outputs": [],
      "source": [
        "# query = \"What are some items that Pirkko Koskitalo is likely to buy next? What incentives can I put in place to ensure he orders more?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "query = random.choice(sample_queries)\n",
        "\n",
        "print(query)"
      ],
      "metadata": {
        "id": "XzH2uVrX02vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgpTn-j-MLAN"
      },
      "outputs": [],
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyU4z3JEMLEb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "raw_query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCL3shOvOTZg"
      },
      "outputs": [],
      "source": [
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=3, include_metadata=True, namespace=namespace)\n",
        "# top_k=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b613bS_gMLG5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "top_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_IO4A1bPRri"
      },
      "outputs": [],
      "source": [
        "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5ndG43QPVjR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-udZviyaPVlz"
      },
      "outputs": [],
      "source": [
        "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C9IRldxPRt4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hTsPfASy9UQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "Your name is Arven.\n",
        "\n",
        "You are the official AI assistant for Aven, a financial services and fintech company. Your role is to provide helpful, accurate, and trustworthy information about Aven's products, services, and company to customers and prospective customers.\n",
        "\n",
        "Core Guidelines:\n",
        "\n",
        "1. Answer Based on Provided Data\n",
        "- ONLY answer questions using information from the context data provided to you\n",
        "- If the provided context doesn't contain sufficient information to answer a question, clearly state: \"I don't have enough information in my current knowledge base to answer that question accurately. For the most up-to-date information, please contact Aven customer support or visit our website.\"\n",
        "- Never make up or infer information that isn't explicitly stated in the provided context\n",
        "\n",
        "2. Tone and Voice\n",
        "- Professional yet approachable: Sound knowledgeable but not overly technical\n",
        "- Helpful and customer-focused: Prioritize solving the user's problem or answering their question\n",
        "- Trustworthy: Be transparent about limitations and always provide accurate information\n",
        "- Concise but complete: Give thorough answers without being unnecessarily verbose\n",
        "\n",
        "3. Response Structure\n",
        "For each response:\n",
        "1. Start with a clear, direct answer to the user's question\n",
        "2. Provide relevant context and details from the provided data\n",
        "3. Suggest relevant actions the user can take (when appropriate)\n",
        "\n",
        "4. Specific Topics to Handle\n",
        "\n",
        "Products & Services:\n",
        "- Explain Aven's financial products and services clearly\n",
        "- Highlight key features and benefits\n",
        "- Compare different options when relevant\n",
        "\n",
        "Eligibility & Applications:\n",
        "- Provide clear information about requirements\n",
        "- Guide users through application processes\n",
        "- Be transparent about approval criteria\n",
        "\n",
        "Fees & Pricing:\n",
        "- Give accurate, up-to-date fee information\n",
        "- Explain any conditions or variables\n",
        "- Be transparent about all costs\n",
        "\n",
        "Security & Trust:\n",
        "- Emphasize Aven's security measures and regulatory compliance\n",
        "- Address customer concerns about safety and legitimacy\n",
        "- Provide reassurance backed by facts\n",
        "\n",
        "Customer Support:\n",
        "- Direct users to appropriate support channels\n",
        "- Provide contact information and hours\n",
        "- Set clear expectations for response times\n",
        "\n",
        "5. What NOT to Do\n",
        "- Don't provide financial advice or recommendations beyond explaining Aven's products\n",
        "- Don't make promises about approval, rates, or terms that aren't guaranteed\n",
        "- Don't share sensitive customer information or ask for personal details\n",
        "- Don't speculate about future products or company plans not mentioned in your data\n",
        "- Don't disparage competitors - focus on Aven's strengths\n",
        "- Don't provide information about other companies unless it's in your provided context\n",
        "\n",
        "6. When You Don't Know\n",
        "If you encounter questions about:\n",
        "- Specific account issues: Direct to customer support\n",
        "- Information not in your data: Clearly state limitations and suggest official channels\n",
        "- Technical problems: Provide troubleshooting steps if available, otherwise direct to support\n",
        "- Legal or regulatory questions: Direct to official documentation or support\n",
        "\n",
        "Example Response Format\n",
        "\n",
        "User Question: \"What credit score do I need for an Aven card?\"\n",
        "\n",
        "Good Response:\n",
        "\"Aven typically considers applicants with credit scores of [specific range if provided in context]. However, credit score is just one factor in our approval process - we also consider [other factors mentioned in context].\n",
        "\n",
        "Aven focuses on [unique approach mentioned in data]. This means that even if your credit score is [relevant details from context].\n",
        "\n",
        "To apply and get a personalized assessment, you can [application process from context]. For specific questions about your eligibility, I recommend contacting Aven customer support at [contact info from context].\"\n",
        "\n",
        "Remember:\n",
        "- You represent Aven - be professional and helpful\n",
        "- Accuracy is more important than being comprehensive\n",
        "- When in doubt, direct users to official channels\n",
        "- Always prioritize the customer's needs and experience\n",
        "- Use only the information provided in your knowledge base\n",
        "\n",
        "Your goal is to be the most helpful, accurate, and trustworthy source of information about Aven while staying within the bounds of your provided data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O4mXn2Eb9_ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WdCT44fQAty"
      },
      "outputs": [],
      "source": [
        "# system_prompt = f\"\"\"You are an expert at understanding and analyzing company data - particularly shipping orders, purchase orders, invoices, and inventory reports.\n",
        "\n",
        "# Answer any questions I have, based on the data provided. Always consider all of the context provided when forming a response.\n",
        "# \"\"\"\n",
        "\n",
        "llm_response = groq_client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kezebsTcPrs3"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tTAtCQzUAyZ"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ2Y6R5DPru0"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, WebBaseLoader, YoutubeLoader, DirectoryLoader, TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import os\n",
        "from groq import Groq\n",
        "from exa_py import Exa\n",
        "import hashlib\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "openai_client = OpenAI()\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "exa_api_key = userdata.get(\"EXA_API_KEY\")\n",
        "os.environ['EXA_API_KEY'] = exa_api_key\n",
        "\n",
        "## create embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "## initialize Groq client\n",
        "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "\n",
        "\n",
        "## Calculate sentence similarity with embeddings\n",
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)\n",
        "\n",
        "\n",
        "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
        "    # Get embeddings for both sentences\n",
        "    embedding1 = np.array(get_huggingface_embeddings(sentence1))\n",
        "    embedding2 = np.array(get_huggingface_embeddings(sentence2))\n",
        "\n",
        "    # Reshape embeddings for cosine_similarity function\n",
        "    embedding1 = embedding1.reshape(1, -1)\n",
        "    embedding2 = embedding2.reshape(1, -1)\n",
        "\n",
        "    print(\"Embedding for Sentence 1:\", embedding1)\n",
        "    print(\"\\nEmbedding for Sentence 2:\", embedding2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(embedding1, embedding2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"I like running to the park\"\n",
        "sentence2 = \"I like running to the office\"\n",
        "\n",
        "\n",
        "similarity = cosine_similarity_between_sentences(sentence1, sentence2)\n",
        "print(f\"\\n\\nCosine similarity between '{sentence1}' and '{sentence2}': {similarity:.4f}\")\n",
        "\n",
        "## initialize Exa client\n",
        "exa = Exa(api_key = os.getenv('EXA_API_KEY'))\n",
        "\n",
        "## Get Exa search results\n",
        "company_name = \"Aven\"\n",
        "num_results = 100\n",
        "all_results = []\n",
        "\n",
        "# Search queries for different types of content\n",
        "search_queries = [\n",
        "    f'site:aven.com aven card',\n",
        "    f'site:aven.com \"aven card\" support articles',\n",
        "    f'site:aven.com \"aven card\" education how it works',\n",
        "    f'site:aven.com \"aven card\" reviews and testimonials',\n",
        "    f'site:aven.com \"aven card\" about us'\n",
        "    # f'news articles blogs about \"{company_name}\" Card the financial services fintech company',\n",
        "    # f\"{company_name} financial services fintech\",\n",
        "    # f\"{company_name} company profile information about\",\n",
        "    # f\"{company_name} products services offerings\"\n",
        "]\n",
        "\n",
        "for query in search_queries:\n",
        "    try:\n",
        "        results = exa.search_and_contents(\n",
        "            query=query,\n",
        "            type=\"neural\",\n",
        "            num_results=num_results // len(search_queries),\n",
        "            text=True,\n",
        "            highlights=True,\n",
        "            use_autoprompt=True,\n",
        "            start_published_date=\"2023-01-01\"\n",
        "        )\n",
        "        all_results.extend(results.results)\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching for query '{query}': {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Total results collected: {len(all_results)}\")\n",
        "print(all_results)\n",
        "\n",
        "\n",
        "## initialize Pinecone\n",
        "index_name = \"arven\"\n",
        "namespace = \"company-documents\"\n",
        "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
        "\n",
        "## insert data into pinecone\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import hashlib\n",
        "\n",
        "# Initialize text splitter for chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,      # Smaller chunks\n",
        "    chunk_overlap=200,    # Some overlap between chunks\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "document_data = []\n",
        "\n",
        "print(f\"Processing {len(all_results)} Exa results with chunking...\")\n",
        "\n",
        "for i, exa_result in enumerate(all_results):\n",
        "    try:\n",
        "        # Extract data from Exa result\n",
        "        document_content = getattr(exa_result, 'text', '') or getattr(exa_result, 'content', '')\n",
        "        document_source = getattr(exa_result, 'url', 'unknown')\n",
        "        title = getattr(exa_result, 'title', 'Unknown Title')\n",
        "        published_date = getattr(exa_result, 'published_date', '')\n",
        "\n",
        "        # Skip if no content\n",
        "        if not document_content or len(document_content.strip()) < 100:\n",
        "            continue\n",
        "\n",
        "        # Create unique content hash\n",
        "        content_hash = hashlib.md5(document_content.encode()).hexdigest()[:8]\n",
        "\n",
        "        # Split content into chunks\n",
        "        chunks = text_splitter.split_text(document_content)\n",
        "\n",
        "        # Process each chunk\n",
        "        for chunk_idx, chunk in enumerate(chunks):\n",
        "            if len(chunk.strip()) < 50:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            # Create MINIMAL metadata (this is the key!)\n",
        "            minimal_metadata = {\n",
        "                \"source\": document_source[:200],  # Truncate URL if very long\n",
        "                \"title\": title[:100],             # Truncate title\n",
        "                \"chunk\": chunk_idx,               # Chunk index\n",
        "                \"doc_id\": f\"{content_hash}_{chunk_idx}\",  # Unique ID\n",
        "                \"company\": company_name,\n",
        "                \"date\": str(published_date)[:10] if published_date else \"\",  # Just date part\n",
        "            }\n",
        "\n",
        "            # Create simple page content (NO extra formatting that adds size)\n",
        "            page_content = chunk  # Just the chunk content, no extra wrapper\n",
        "\n",
        "            doc = Document(\n",
        "                page_content=page_content,\n",
        "                metadata=minimal_metadata\n",
        "            )\n",
        "\n",
        "            document_data.append(doc)\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(all_results)} results...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing result {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Created {len(document_data)} document chunks\")\n",
        "\n",
        "# Test metadata size on a few samples\n",
        "import json\n",
        "\n",
        "def get_metadata_size(doc):\n",
        "    return len(json.dumps(doc.metadata).encode('utf-8'))\n",
        "\n",
        "print(\"\\n🔍 Checking metadata sizes:\")\n",
        "for i in range(min(5, len(document_data))):\n",
        "    size = get_metadata_size(document_data[i])\n",
        "    content_size = len(document_data[i].page_content)\n",
        "    print(f\"Doc {i+1}: metadata={size}B, content={content_size}B\")\n",
        "\n",
        "    if size > 40000:\n",
        "        print(f\"❌ STILL TOO LARGE: {size} bytes\")\n",
        "    else:\n",
        "        print(f\"✅ OK: {size} bytes\")\n",
        "\n",
        "# If you're STILL getting errors, try this ultra-minimal version:\n",
        "def create_ultra_minimal_docs(all_results):\n",
        "    \"\"\"Ultra minimal approach - absolutely bare minimum metadata\"\"\"\n",
        "    ultra_minimal_docs = []\n",
        "\n",
        "    for i, exa_result in enumerate(all_results):\n",
        "        try:\n",
        "            content = getattr(exa_result, 'text', '') or getattr(exa_result, 'content', '')\n",
        "            if not content or len(content.strip()) < 100:\n",
        "                continue\n",
        "\n",
        "            # Split into chunks\n",
        "            chunks = text_splitter.split_text(content)\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                if len(chunk.strip()) < 50:\n",
        "                    continue\n",
        "\n",
        "                # ABSOLUTE MINIMUM metadata\n",
        "                doc = Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata={\n",
        "                        \"id\": f\"doc_{i}_{chunk_idx}\",\n",
        "                        \"company\": company_name\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                ultra_minimal_docs.append(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return ultra_minimal_docs\n",
        "\n",
        "# Uncomment this if you're still getting metadata errors:\n",
        "# print(\"Using ultra-minimal metadata approach...\")\n",
        "# document_data = create_ultra_minimal_docs(all_results)\n",
        "# print(f\"Created {len(document_data)} ultra-minimal documents\")\n",
        "\n",
        "print(f\"\\n✅ Ready to upload {len(document_data)} documents to Pinecone\")\n",
        "print(\"Now try running the PineconeVectorStore.from_documents() again\")\n",
        "\n",
        "\n",
        "## insert documents into Pinecone\n",
        "# Insert documents into Pinecone\n",
        "vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
        "    document_data,\n",
        "    embeddings,\n",
        "    index_name=index_name,\n",
        "    namespace=namespace\n",
        ")\n",
        "\n",
        "\n",
        "## perform rag\n",
        "def perform_rag(query):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    query_embedding = np.array(raw_query_embedding)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    system_prompt = f\"\"\"Your name is Arven.\n",
        "\n",
        "You are the official AI Customer Support agent for Aven, a financial services and fintech company. Your role is to provide helpful, accurate, and trustworthy information about Aven's products, services, and company to customers and prospective customers.\n",
        "\n",
        "Core Guidelines:\n",
        "\n",
        "1. Answer Based on Provided Data\n",
        "- ONLY answer questions using information from the context data provided to you\n",
        "- If the provided context doesn't contain sufficient information to answer a question, clearly state: \"I don't have enough information in my current knowledge base to answer that question accurately. For the most up-to-date information, please contact Aven customer support or visit our website.\"\n",
        "- Never make up or infer information that isn't explicitly stated in the provided context\n",
        "\n",
        "2. Tone and Voice\n",
        "- Professional yet approachable: Sound knowledgeable but not overly technical\n",
        "- Helpful and customer-focused: Prioritize solving the user's problem or answering their question\n",
        "- Trustworthy: Be transparent about limitations and always provide accurate information\n",
        "- Concise but complete: Give thorough answers without being unnecessarily verbose\n",
        "\n",
        "3. Response Structure\n",
        "For each response:\n",
        "1. Start with a clear, direct answer to the user's question\n",
        "2. Provide relevant context and details from the provided data\n",
        "3. Suggest relevant actions the user can take (when appropriate)\n",
        "\n",
        "4. Specific Topics to Handle\n",
        "\n",
        "Products & Services:\n",
        "- Explain Aven's financial products and services clearly\n",
        "- Highlight key features and benefits\n",
        "- Compare different options when relevant\n",
        "\n",
        "Eligibility & Applications:\n",
        "- Provide clear information about requirements\n",
        "- Guide users through application processes\n",
        "- Be transparent about approval criteria\n",
        "\n",
        "Fees & Pricing:\n",
        "- Give accurate, up-to-date fee information\n",
        "- Explain any conditions or variables\n",
        "- Be transparent about all costs\n",
        "\n",
        "Security & Trust:\n",
        "- Emphasize Aven's security measures and regulatory compliance\n",
        "- Address customer concerns about safety and legitimacy\n",
        "- Provide reassurance backed by facts\n",
        "\n",
        "Customer Support:\n",
        "- Direct users to appropriate support channels\n",
        "- Provide contact information and hours\n",
        "- Set clear expectations for response times\n",
        "\n",
        "5. What NOT to Do\n",
        "- Don't provide financial advice or recommendations beyond explaining Aven's products\n",
        "- Don't make promises about approval, rates, or terms that aren't guaranteed\n",
        "- Don't share sensitive customer information or ask for personal details\n",
        "- Don't speculate about future products or company plans not mentioned in your data\n",
        "- Don't disparage competitors - focus on Aven's strengths\n",
        "- Don't provide information about other companies unless it's in your provided context\n",
        "\n",
        "6. When You Don't Know\n",
        "If you encounter questions about:\n",
        "- Specific account issues: Direct to customer support\n",
        "- Information not in your data: Clearly state limitations and suggest official channels\n",
        "- Technical problems: Provide troubleshooting steps if available, otherwise direct to support\n",
        "- Legal or regulatory questions: Direct to official documentation or support\n",
        "\n",
        "Example Response Format\n",
        "\n",
        "User Question: \"What credit score do I need for an Aven card?\"\n",
        "\n",
        "Good Response:\n",
        "\"Aven typically considers applicants with credit scores of [specific range if provided in context]. However, credit score is just one factor in our approval process - we also consider [other factors mentioned in context].\n",
        "\n",
        "Aven focuses on [unique approach mentioned in data]. This means that even if your credit score is [relevant details from context].\n",
        "\n",
        "To apply and get a personalized assessment, you can [application process from context]. For specific questions about your eligibility, I recommend contacting Aven customer support at [contact info from context].\"\n",
        "\n",
        "Remember:\n",
        "- You represent Aven - be professional and helpful\n",
        "- Accuracy is more important than being comprehensive\n",
        "- When in doubt, direct users to official channels\n",
        "- Always prioritize the customer's needs and experience\n",
        "- Use only the information provided in your knowledge base\n",
        "\n",
        "Your goal is to be the most helpful, accurate, and trustworthy source of information about Aven while staying within the bounds of your provided data.\n",
        "\"\"\"\n",
        "\n",
        "    res = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\", # llama-3.1-70b-versatile\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return res.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFqRJjKYLB13"
      },
      "outputs": [],
      "source": [
        "# If you have access to the o1 model through the OpenAI API, you can use this function to compare the quality of responses\n",
        "# def perform_rag_openai(query):\n",
        "#     raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "#     query_embedding = np.array(raw_query_embedding)\n",
        "\n",
        "#     top_matches = pinecone_index.query(vector=query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)\n",
        "\n",
        "#     # Get the list of retrieved texts\n",
        "#     contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "#     augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "#     # Modify the prompt below as need to improve the response quality\n",
        "#     system_prompt = f\"\"\"You are an expert at understanding and analyzing company data - particularly shipping orders, purchase orders, invoices, and inventory reports.\n",
        "\n",
        "#     Answer any questions I have, based on the data provided. Always consider all parts of the context provided when forming a response.\n",
        "#     \"\"\"\n",
        "\n",
        "#     res = openai_client.chat.completions.create(\n",
        "#         model=\"o1-preview\",\n",
        "#         messages=[\n",
        "#             {\"role\": \"user\", \"content\": f\"{system_prompt} {augmented_query}\"}\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "#     return res.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSests6uWRU1"
      },
      "outputs": [],
      "source": [
        "# response = perform_rag(\"What are some trends with Ricardo Adocicados purchase orders?\")\n",
        "query = random.choice(sample_queries)\n",
        "print(query)\n",
        "print(\"+__________+\")\n",
        "response = perform_rag(query)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtXn9S5TLIO3"
      },
      "outputs": [],
      "source": [
        "# response = perform_rag_openai(\"What are some trends with Ricardo Adocicados purchase orders?\")\n",
        "\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzfIFuNCUlvk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHjyXFqVfne"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoKC4-DXVfp4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UFp3fV8YVaPX",
        "7YA0Wj0X-QB2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}