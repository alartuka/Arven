{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDW/3THGcZQiRBVZ7PGBOX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aven Website Comprehensive Crawler\n",
        "### Run this notebook in Google Colab to crawl and store Aven website content in Pinecone"
      ],
      "metadata": {
        "id": "NQa7EcBbBk9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XOcHSHNxBtrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "nd_Fuuo5B4ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv sentence-transformers scikit-learn pinecone-client exa-py requests numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pallZLwDB9uE",
        "outputId": "ff7ca984-e475-4bae-85cd-8cfabc42b0cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: exa-py in /usr/local/lib/python3.11/dist-packages (1.14.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.7.14)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.5.0)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from exa-py) (0.28.1)\n",
            "Requirement already satisfied: openai>=1.48 in /usr/local/lib/python3.11/dist-packages (from exa-py) (1.97.1)\n",
            "Requirement already satisfied: pydantic>=2.10.6 in /usr/local/lib/python3.11/dist-packages (from exa-py) (2.11.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->exa-py) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "0Qp8YrfOCKYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import hashlib\n",
        "import time\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from pinecone import Pinecone\n",
        "from exa_py import Exa\n"
      ],
      "metadata": {
        "id": "ytLJOjuBCWub"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Variables Setup"
      ],
      "metadata": {
        "id": "9rcKfmPtC50z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "exa_api_key = userdata.get(\"EXA_API_KEY\")\n",
        "os.environ['EXA_API_KEY'] = exa_api_key\n"
      ],
      "metadata": {
        "id": "nW_SsnGkCeNL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Clients\n"
      ],
      "metadata": {
        "id": "lRuUB0q9DZSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize sentence transformer model\n",
        "sentence_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize Exa client\n",
        "exa_client = Exa(api_key=exa_api_key)\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "index_name = \"arven\"\n",
        "pinecone_index = pc.Index(index_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "OPHsKx5mDlt3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Text Embeddings"
      ],
      "metadata": {
        "id": "5__5nsc-HP4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text):\n",
        "    \"\"\"Get embeddings for text using the loaded model\"\"\"\n",
        "    global sentence_model\n",
        "    return sentence_model.encode(text)\n",
        ""
      ],
      "metadata": {
        "id": "4V2EMMbwEaoS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Text into Chunks"
      ],
      "metadata": {
        "id": "aM8XvwX7HW84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_chunks(text, max_chunk_size=1000, overlap=100):\n",
        "    \"\"\"Split text into overlapping chunks for better retrieval\"\"\"\n",
        "    if len(text) <= max_chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + max_chunk_size\n",
        "\n",
        "        # if we're not at the end, try to break at a sentence or word boundary\n",
        "        if end < len(text):\n",
        "            # look for sentence ending\n",
        "            sentence_break = text.rfind('.', start, end)\n",
        "            if sentence_break > start + max_chunk_size // 2:\n",
        "                end = sentence_break + 1\n",
        "            else:\n",
        "                # look for word boundary\n",
        "                word_break = text.rfind(' ', start, end)\n",
        "                if word_break > start + max_chunk_size // 2:\n",
        "                    end = word_break\n",
        "\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        # move start position with overlap\n",
        "        start = max(start + max_chunk_size - overlap, end)\n",
        "\n",
        "        # prevent infinite loop\n",
        "        if start >= len(text):\n",
        "            break\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "LkXue4rXEs3I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Aven's Sitemap URLs"
      ],
      "metadata": {
        "id": "NVUjf-VbHnpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sitemap_urls():\n",
        "    \"\"\"Fetch and parse sitemap.xml to get all URLs\"\"\"\n",
        "    sitemap_url = \"https://aven.com/sitemap.xml\"\n",
        "    urls = []\n",
        "\n",
        "    try:\n",
        "        print(f\"üîç Fetching sitemap from {sitemap_url}...\")\n",
        "        response = requests.get(sitemap_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # parse XML\n",
        "        root = ET.fromstring(response.content)\n",
        "\n",
        "        # handle different sitemap formats\n",
        "        # standard sitemap namespace\n",
        "        ns = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "        # try to find URL elements with namespace\n",
        "        url_elements = root.findall('.//sitemap:url/sitemap:loc', ns)\n",
        "\n",
        "        # if no namespace URLs found, try without namespace\n",
        "        if not url_elements:\n",
        "            url_elements = root.findall('.//url/loc')\n",
        "\n",
        "        # if still no URLs, try different approach for sitemap index\n",
        "        if not url_elements:\n",
        "            url_elements = root.findall('.//loc')\n",
        "\n",
        "        for url_elem in url_elements:\n",
        "            url = url_elem.text.strip()\n",
        "            if url and url.startswith('https://aven.com'):\n",
        "                urls.append(url)\n",
        "\n",
        "        print(f\">>> Found {len(urls)} URLs in sitemap\")\n",
        "        return urls\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\">>> Error fetching sitemap: {e}\")\n",
        "        print(\">>> Falling back to base URL crawling...\")\n",
        "        return [\"https://aven.com\"]"
      ],
      "metadata": {
        "id": "KUnqJ51KFBxM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl Aven's Website with Exa and Store in Pinecone"
      ],
      "metadata": {
        "id": "o2oDrJSXFsCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_and_store_website():\n",
        "    \"\"\"Crawl Aven website using Exa and store in Pinecone - COMPREHENSIVE SITEMAP\"\"\"\n",
        "    global exa_client, pinecone_index\n",
        "\n",
        "    try:\n",
        "        print(\"üï∑Ô∏è Starting comprehensive website crawl from sitemap...\")\n",
        "\n",
        "        # get all URLs from sitemap\n",
        "        sitemap_urls = get_sitemap_urls()\n",
        "\n",
        "        # process URLs in batches to avoid API limits\n",
        "        batch_size = 25  # process 25 URLs at a time\n",
        "        all_processed_urls = []\n",
        "        vectors_to_upsert = []\n",
        "        total_processed_count = 0\n",
        "\n",
        "        for i in range(0, len(sitemap_urls), batch_size):\n",
        "            batch_urls = sitemap_urls[i:i + batch_size]\n",
        "            batch_num = i // batch_size + 1\n",
        "            total_batches = (len(sitemap_urls) + batch_size - 1) // batch_size\n",
        "\n",
        "            print(f\">>> Processing batch {batch_num}/{total_batches} ({len(batch_urls)} URLs)...\")\n",
        "\n",
        "            try:\n",
        "                # crawl this batch of URLs using Exa\n",
        "                response = exa_client.get_contents(\n",
        "                    batch_urls,           # list of URLs to crawl\n",
        "                    text=True,           # get the full text\n",
        "                    subpages=0           # don't crawl subpages since we have the sitemap\n",
        "                )\n",
        "\n",
        "                batch_results = response.results\n",
        "                print(f\"üîç Got {len(batch_results)} results from Exa for this batch\")\n",
        "\n",
        "                # process each page in the batch\n",
        "                for result in batch_results:\n",
        "                    try:\n",
        "                        # extract content and metadata\n",
        "                        url = result.url\n",
        "                        title = getattr(result, 'title', 'Unknown Title')\n",
        "                        text_content = getattr(result, 'text', '')\n",
        "\n",
        "                        # skip if no content\n",
        "                        if not text_content or len(text_content.strip()) < 50:\n",
        "                            print(f\">>>  Skipping {url} - insufficient content\")\n",
        "                            continue\n",
        "\n",
        "                        # verify it's from Aven domain\n",
        "                        domain = urlparse(url).netloc.lower()\n",
        "                        if not (domain == 'aven.com' or domain.endswith('.aven.com')):\n",
        "                            print(f\">>>  Skipping {url} - not from Aven domain\")\n",
        "                            continue\n",
        "\n",
        "                        # split content into chunks (for better retrieval)\n",
        "                        chunks = split_text_into_chunks(text_content, max_chunk_size=1000, overlap=100)\n",
        "\n",
        "                        for chunk_idx, chunk in enumerate(chunks):\n",
        "                            # generate embedding for the chunk\n",
        "                            chunk_embedding = get_huggingface_embeddings(chunk)\n",
        "\n",
        "                            # create unique ID for this chunk\n",
        "                            chunk_id = hashlib.md5(f\"{url}_{chunk_idx}_{chunk[:100]}\".encode()).hexdigest()\n",
        "\n",
        "                            # prepare metadata\n",
        "                            metadata = {\n",
        "                                'source': url,\n",
        "                                'title': title,\n",
        "                                'text': chunk,\n",
        "                                'page_content': chunk,  # alternative field name\n",
        "                                'content': chunk,       # another alternative field name\n",
        "                                'domain': domain,\n",
        "                                'verified_aven': True,\n",
        "                                'company': 'Aven',\n",
        "                                'chunk_index': chunk_idx,\n",
        "                                'total_chunks': len(chunks),\n",
        "                                'crawl_timestamp': int(time.time()),\n",
        "                                'source_type': 'exa_sitemap_crawl_colab',\n",
        "                                'batch_number': batch_num,\n",
        "                                'crawl_method': 'colab_standalone'\n",
        "                            }\n",
        "\n",
        "                            # prepare vector for upsert\n",
        "                            vector_data = {\n",
        "                                'id': chunk_id,\n",
        "                                'values': chunk_embedding.tolist(),\n",
        "                                'metadata': metadata\n",
        "                            }\n",
        "\n",
        "                            vectors_to_upsert.append(vector_data)\n",
        "\n",
        "                        all_processed_urls.append(url)\n",
        "                        total_processed_count += 1\n",
        "                        print(f\">>> Processed {url} - {len(chunks)} chunks\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\">>> Error processing {getattr(result, 'url', 'unknown URL')}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # small delay between batches to be respectful to APIs\n",
        "                if i + batch_size < len(sitemap_urls):\n",
        "                    print(\">>>  Waiting 2 seconds before next batch...\")\n",
        "                    time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\">>> Error processing batch {batch_num}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # upsert all vectors to Pinecone in batches\n",
        "        if vectors_to_upsert:\n",
        "            print(f\">>> Upserting {len(vectors_to_upsert)} vectors to Pinecone...\")\n",
        "\n",
        "            pinecone_batch_size = 100\n",
        "            for i in range(0, len(vectors_to_upsert), pinecone_batch_size):\n",
        "                batch = vectors_to_upsert[i:i + pinecone_batch_size]\n",
        "                pinecone_index.upsert(\n",
        "                    vectors=batch,\n",
        "                    namespace=\"company-documents\"\n",
        "                )\n",
        "                print(f\">>> Upserted batch {i//pinecone_batch_size + 1}/{(len(vectors_to_upsert) + pinecone_batch_size - 1)//pinecone_batch_size}\")\n",
        "\n",
        "            print(f\">>> Successfully crawled and stored {total_processed_count} pages ({len(vectors_to_upsert)} chunks)\")\n",
        "            print(f\">>> Processed URLs: {len(all_processed_urls)}/{len(sitemap_urls)}\")\n",
        "\n",
        "            # summary statistics\n",
        "            return {\n",
        "                'success': True,\n",
        "                'pages_processed': total_processed_count,\n",
        "                'chunks_stored': len(vectors_to_upsert),\n",
        "                'total_sitemap_urls': len(sitemap_urls),\n",
        "                'processed_urls': all_processed_urls,\n",
        "                'timestamp': int(time.time())\n",
        "            }\n",
        "        else:\n",
        "            print(\">>>  No content was processed and stored\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': 'No content was processed and stored',\n",
        "                'pages_processed': 0,\n",
        "                'chunks_stored': 0,\n",
        "                'total_sitemap_urls': len(sitemap_urls)\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\">>> Website crawl error: {e}\")\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'pages_processed': 0,\n",
        "            'chunks_stored': 0\n",
        "        }"
      ],
      "metadata": {
        "id": "oEuD9rkXFvJ7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone Stored Data Verification"
      ],
      "metadata": {
        "id": "9VqE9vprF-pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_stored_data():\n",
        "    \"\"\"Verify that data was stored correctly in Pinecone\"\"\"\n",
        "    global pinecone_index\n",
        "\n",
        "    try:\n",
        "        print(\">>> Verifying stored data...\")\n",
        "\n",
        "        # Get index stats\n",
        "        stats = pinecone_index.describe_index_stats()\n",
        "        namespace_stats = stats.namespaces.get(\"company-documents\", {})\n",
        "        vector_count = namespace_stats.get('vector_count', 0)\n",
        "\n",
        "        print(f\">>> Total vectors in 'company-documents' namespace: {vector_count}\")\n",
        "\n",
        "        # Sample some data\n",
        "        import random\n",
        "        random_vector = [random.random() for _ in range(384)]\n",
        "\n",
        "        results = pinecone_index.query(\n",
        "            vector=random_vector,\n",
        "            top_k=5,\n",
        "            namespace=\"company-documents\",\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        print(f\">>> Sample of stored data:\")\n",
        "        aven_count = 0\n",
        "\n",
        "        for i, match in enumerate(results.matches):\n",
        "            metadata = match.metadata\n",
        "            source = metadata.get('source', 'Unknown')\n",
        "            title = metadata.get('title', 'Unknown')[:50]\n",
        "            domain = urlparse(source).netloc.lower() if source else 'unknown'\n",
        "            is_aven = domain == 'aven.com' or domain.endswith('.aven.com')\n",
        "\n",
        "            if is_aven:\n",
        "                aven_count += 1\n",
        "\n",
        "            print(f\"  {i+1}. {title} | {domain} | {'>> Aven' if is_aven else '>> Non-Aven'}\")\n",
        "\n",
        "        verification_result = {\n",
        "            'total_vectors': vector_count,\n",
        "            'sample_size': len(results.matches),\n",
        "            'aven_sources': aven_count,\n",
        "            'aven_percentage': (aven_count / len(results.matches) * 100) if results.matches else 0,\n",
        "            'is_properly_filtered': aven_count == len(results.matches)\n",
        "        }\n",
        "\n",
        "        print(f\">>> Verification complete: {aven_count}/{len(results.matches)} sources are from Aven\")\n",
        "        return verification_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\">>> Verification failed: {e}\")\n",
        "        return {'error': str(e)}"
      ],
      "metadata": {
        "id": "TcylPv51GBAD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN - Comprehensive Crawling Execution"
      ],
      "metadata": {
        "id": "-SKRAat3GkBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\">>> Starting comprehensive crawl of Aven website...\")\n",
        "print(\"This may take several minutes depending on the website size.\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute the crawl\n",
        "crawl_results = crawl_and_store_website()\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÅ CRAWL COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if crawl_results['success']:\n",
        "    print(f\"‚úÖ Success! Crawled and stored website content.\")\n",
        "    print(f\"üìä Pages processed: {crawl_results['pages_processed']}\")\n",
        "    print(f\"üì¶ Chunks stored: {crawl_results['chunks_stored']}\")\n",
        "    print(f\"üåê Total sitemap URLs: {crawl_results['total_sitemap_urls']}\")\n",
        "    print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds\")\n",
        "    print(f\"üïê Completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Run verification\n",
        "    print(\"\\n>>> Running verification...\")\n",
        "    verification = verify_stored_data()\n",
        "    if 'error' not in verification:\n",
        "        print(f\"üìä Verification Results:\")\n",
        "        print(f\"   Total vectors: {verification['total_vectors']}\")\n",
        "        print(f\"   Aven sources: {verification['aven_percentage']:.1f}%\")\n",
        "        print(f\"   Filtering status: {'‚úÖ Success' if verification['is_properly_filtered'] else '‚ö†Ô∏è  Some non-Aven content detected'}\")\n",
        "else:\n",
        "    print(f\"‚ùå Crawl failed: {crawl_results.get('error', 'Unknown error')}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWFDk9CeGie_",
        "outputId": "4137c961-8c01-48ee-d0d6-ccf250bbe028"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Starting comprehensive crawl of Aven website...\n",
            "This may take several minutes depending on the website size.\n",
            "\n",
            "============================================================\n",
            "üï∑Ô∏è Starting comprehensive website crawl from sitemap...\n",
            "üîç Fetching sitemap from https://aven.com/sitemap.xml...\n",
            ">>> Found 38 URLs in sitemap\n",
            ">>> Processing batch 1/2 (25 URLs)...\n",
            "üîç Got 5 results from Exa for this batch\n",
            ">>> Processed https://www.aven.com/ - 2 chunks\n",
            ">>> Processed https://www.aven.com/about/ - 2 chunks\n",
            ">>> Processed https://www.aven.com/advisor - 2 chunks\n",
            ">>> Processed https://www.aven.com/advisorApp - 2 chunks\n",
            ">>> Processed https://www.aven.com/education - 45 chunks\n",
            ">>>  Waiting 2 seconds before next batch...\n",
            ">>> Processing batch 2/2 (13 URLs)...\n",
            "üîç Got 2 results from Exa for this batch\n",
            ">>> Processed https://www.aven.com/support/ - 57 chunks\n",
            ">>> Processed https://www.aven.com/press/series-d/ - 20 chunks\n",
            ">>> Upserting 130 vectors to Pinecone...\n",
            ">>> Upserted batch 1/2\n",
            ">>> Upserted batch 2/2\n",
            ">>> Successfully crawled and stored 7 pages (130 chunks)\n",
            ">>> Processed URLs: 7/38\n",
            "\n",
            "============================================================\n",
            "üèÅ CRAWL COMPLETED!\n",
            "============================================================\n",
            "‚úÖ Success! Crawled and stored website content.\n",
            "üìä Pages processed: 7\n",
            "üì¶ Chunks stored: 130\n",
            "üåê Total sitemap URLs: 38\n",
            "‚è±Ô∏è  Duration: 28.7 seconds\n",
            "üïê Completed at: 2025-07-27 23:05:24\n",
            "\n",
            ">>> Running verification...\n",
            ">>> Verifying stored data...\n",
            ">>> Total vectors in 'company-documents' namespace: 2\n",
            ">>> Sample of stored data:\n",
            "  1. Aven HELOC Card: A Credit Card backed by Home Equi | www.aven.com | >> Aven\n",
            "  2. Aven HELOC Card: A Credit Card backed by Home Equi | www.aven.com | >> Aven\n",
            ">>> Verification complete: 2/2 sources are from Aven\n",
            "üìä Verification Results:\n",
            "   Total vectors: 2\n",
            "   Aven sources: 100.0%\n",
            "   Filtering status: ‚úÖ Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional"
      ],
      "metadata": {
        "id": "UN348bWoG4LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MANUAL VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "# Uncomment and run this cell if you want to manually verify the stored data\n",
        "\"\"\"\n",
        "print(\"üîç Manual verification of stored data...\")\n",
        "verification_results = verify_stored_data()\n",
        "print(\"\\nVerification Results:\")\n",
        "for key, value in verification_results.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# VIEW SITEMAP URLS\n",
        "# =============================================================================\n",
        "\n",
        "# Uncomment and run this cell if you want to see what URLs were found in the sitemap\n",
        "\"\"\"\n",
        "print(\"üîç Fetching sitemap URLs for inspection...\")\n",
        "urls = get_sitemap_urls()\n",
        "print(f\"\\nFound {len(urls)} URLs in sitemap:\")\n",
        "for i, url in enumerate(urls[:20], 1):  # Show first 20 URLs\n",
        "    print(f\"  {i}. {url}\")\n",
        "if len(urls) > 20:\n",
        "    print(f\"  ... and {len(urls) - 20} more URLs\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LNI2sWmTBhG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}