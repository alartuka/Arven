{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDL7LC+00cCgJlff6OTjbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alartuka/Arven/blob/main/src/backend/aven_site_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aven Website Comprehensive Crawler\n",
        "### Run this notebook in Google Colab to crawl and store Aven website content in Pinecone"
      ],
      "metadata": {
        "id": "NQa7EcBbBk9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XOcHSHNxBtrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "nd_Fuuo5B4ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv sentence-transformers scikit-learn pinecone-client exa-py requests numpy\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pallZLwDB9uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "0Qp8YrfOCKYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import hashlib\n",
        "import time\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from pinecone import Pinecone\n",
        "from exa_py import Exa\n"
      ],
      "metadata": {
        "id": "ytLJOjuBCWub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Variables Setup"
      ],
      "metadata": {
        "id": "9rcKfmPtC50z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "exa_api_key = userdata.get(\"EXA_API_KEY\")\n",
        "os.environ['EXA_API_KEY'] = exa_api_key\n"
      ],
      "metadata": {
        "id": "nW_SsnGkCeNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Clients\n"
      ],
      "metadata": {
        "id": "lRuUB0q9DZSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize sentence transformer model\n",
        "sentence_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize Exa client\n",
        "exa_client = Exa(api_key=exa_api_key)\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "index_name = \"arven\"\n",
        "pinecone_index = pc.Index(index_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "OPHsKx5mDlt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Text Embeddings"
      ],
      "metadata": {
        "id": "5__5nsc-HP4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text):\n",
        "    \"\"\"Get embeddings for text using the loaded model\"\"\"\n",
        "    global sentence_model\n",
        "    return sentence_model.encode(text)\n",
        ""
      ],
      "metadata": {
        "id": "4V2EMMbwEaoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Text into Chunks"
      ],
      "metadata": {
        "id": "aM8XvwX7HW84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_chunks(text, max_chunk_size=1000, overlap=100):\n",
        "    \"\"\"Split text into overlapping chunks for better retrieval\"\"\"\n",
        "    if len(text) <= max_chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + max_chunk_size\n",
        "\n",
        "        # if we're not at the end, try to break at a sentence or word boundary\n",
        "        if end < len(text):\n",
        "            # look for sentence ending\n",
        "            sentence_break = text.rfind('.', start, end)\n",
        "            if sentence_break > start + max_chunk_size // 2:\n",
        "                end = sentence_break + 1\n",
        "            else:\n",
        "                # look for word boundary\n",
        "                word_break = text.rfind(' ', start, end)\n",
        "                if word_break > start + max_chunk_size // 2:\n",
        "                    end = word_break\n",
        "\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        # move start position with overlap\n",
        "        start = max(start + max_chunk_size - overlap, end)\n",
        "\n",
        "        # prevent infinite loop\n",
        "        if start >= len(text):\n",
        "            break\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "LkXue4rXEs3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Aven's Sitemap URLs"
      ],
      "metadata": {
        "id": "NVUjf-VbHnpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sitemap_urls():\n",
        "    \"\"\"Fetch and parse sitemap.xml to get all URLs\"\"\"\n",
        "    sitemap_url = \"https://aven.com/sitemap.xml\"\n",
        "    urls = []\n",
        "\n",
        "    try:\n",
        "        print(f\"🔍 Fetching sitemap from {sitemap_url}...\")\n",
        "        response = requests.get(sitemap_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # parse XML\n",
        "        root = ET.fromstring(response.content)\n",
        "\n",
        "        # handle different sitemap formats\n",
        "        # standard sitemap namespace\n",
        "        ns = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "\n",
        "        # try to find URL elements with namespace\n",
        "        url_elements = root.findall('.//sitemap:url/sitemap:loc', ns)\n",
        "\n",
        "        # if no namespace URLs found, try without namespace\n",
        "        if not url_elements:\n",
        "            url_elements = root.findall('.//url/loc')\n",
        "\n",
        "        # if still no URLs, try different approach for sitemap index\n",
        "        if not url_elements:\n",
        "            url_elements = root.findall('.//loc')\n",
        "\n",
        "        for url_elem in url_elements:\n",
        "            url = url_elem.text.strip()\n",
        "            if url and url.startswith('https://aven.com'):\n",
        "                urls.append(url)\n",
        "\n",
        "        print(f\">>> Found {len(urls)} URLs in sitemap\")\n",
        "        return urls\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\">>> Error fetching sitemap: {e}\")\n",
        "        print(\">>> Falling back to base URL crawling...\")\n",
        "        return [\"https://aven.com\"]"
      ],
      "metadata": {
        "id": "KUnqJ51KFBxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl Aven's Website with Exa and Store in Pinecone"
      ],
      "metadata": {
        "id": "o2oDrJSXFsCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_and_store_website():\n",
        "    \"\"\"Crawl Aven website using Exa and store in Pinecone - COMPREHENSIVE SITEMAP\"\"\"\n",
        "    global exa_client, pinecone_index\n",
        "\n",
        "    try:\n",
        "        print(\"🕷️ Starting comprehensive website crawl from sitemap...\")\n",
        "\n",
        "        # get all URLs from sitemap\n",
        "        sitemap_urls = get_sitemap_urls()\n",
        "\n",
        "        # process URLs in batches to avoid API limits\n",
        "        batch_size = 25  # process 25 URLs at a time\n",
        "        all_processed_urls = []\n",
        "        vectors_to_upsert = []\n",
        "        total_processed_count = 0\n",
        "\n",
        "        for i in range(0, len(sitemap_urls), batch_size):\n",
        "            batch_urls = sitemap_urls[i:i + batch_size]\n",
        "            batch_num = i // batch_size + 1\n",
        "            total_batches = (len(sitemap_urls) + batch_size - 1) // batch_size\n",
        "\n",
        "            print(f\">>> Processing batch {batch_num}/{total_batches} ({len(batch_urls)} URLs)...\")\n",
        "\n",
        "            try:\n",
        "                # crawl this batch of URLs using Exa\n",
        "                response = exa_client.get_contents(\n",
        "                    batch_urls,           # list of URLs to crawl\n",
        "                    text=True,           # get the full text\n",
        "                    subpages=0           # don't crawl subpages since we have the sitemap\n",
        "                )\n",
        "\n",
        "                batch_results = response.results\n",
        "                print(f\"🔍 Got {len(batch_results)} results from Exa for this batch\")\n",
        "\n",
        "                # process each page in the batch\n",
        "                for result in batch_results:\n",
        "                    try:\n",
        "                        # extract content and metadata\n",
        "                        url = result.url\n",
        "                        title = getattr(result, 'title', 'Unknown Title')\n",
        "                        text_content = getattr(result, 'text', '')\n",
        "\n",
        "                        # skip if no content\n",
        "                        if not text_content or len(text_content.strip()) < 50:\n",
        "                            print(f\">>>  Skipping {url} - insufficient content\")\n",
        "                            continue\n",
        "\n",
        "                        # verify it's from Aven domain\n",
        "                        domain = urlparse(url).netloc.lower()\n",
        "                        if not (domain == 'aven.com' or domain.endswith('.aven.com')):\n",
        "                            print(f\">>>  Skipping {url} - not from Aven domain\")\n",
        "                            continue\n",
        "\n",
        "                        # split content into chunks (for better retrieval)\n",
        "                        chunks = split_text_into_chunks(text_content, max_chunk_size=1000, overlap=100)\n",
        "\n",
        "                        for chunk_idx, chunk in enumerate(chunks):\n",
        "                            # generate embedding for the chunk\n",
        "                            chunk_embedding = get_huggingface_embeddings(chunk)\n",
        "\n",
        "                            # create unique ID for this chunk\n",
        "                            chunk_id = hashlib.md5(f\"{url}_{chunk_idx}_{chunk[:100]}\".encode()).hexdigest()\n",
        "\n",
        "                            # prepare metadata\n",
        "                            metadata = {\n",
        "                                'source': url,\n",
        "                                'title': title,\n",
        "                                'text': chunk,\n",
        "                                'page_content': chunk,  # alternative field name\n",
        "                                'content': chunk,       # another alternative field name\n",
        "                                'domain': domain,\n",
        "                                'verified_aven': True,\n",
        "                                'company': 'Aven',\n",
        "                                'chunk_index': chunk_idx,\n",
        "                                'total_chunks': len(chunks),\n",
        "                                'crawl_timestamp': int(time.time()),\n",
        "                                'source_type': 'exa_sitemap_crawl_colab',\n",
        "                                'batch_number': batch_num,\n",
        "                                'crawl_method': 'colab_standalone'\n",
        "                            }\n",
        "\n",
        "                            # prepare vector for upsert\n",
        "                            vector_data = {\n",
        "                                'id': chunk_id,\n",
        "                                'values': chunk_embedding.tolist(),\n",
        "                                'metadata': metadata\n",
        "                            }\n",
        "\n",
        "                            vectors_to_upsert.append(vector_data)\n",
        "\n",
        "                        all_processed_urls.append(url)\n",
        "                        total_processed_count += 1\n",
        "                        print(f\">>> Processed {url} - {len(chunks)} chunks\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\">>> Error processing {getattr(result, 'url', 'unknown URL')}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # small delay between batches to be respectful to APIs\n",
        "                if i + batch_size < len(sitemap_urls):\n",
        "                    print(\">>>  Waiting 2 seconds before next batch...\")\n",
        "                    time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\">>> Error processing batch {batch_num}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # upsert all vectors to Pinecone in batches\n",
        "        if vectors_to_upsert:\n",
        "            print(f\">>> Upserting {len(vectors_to_upsert)} vectors to Pinecone...\")\n",
        "\n",
        "            pinecone_batch_size = 100\n",
        "            for i in range(0, len(vectors_to_upsert), pinecone_batch_size):\n",
        "                batch = vectors_to_upsert[i:i + pinecone_batch_size]\n",
        "                pinecone_index.upsert(\n",
        "                    vectors=batch,\n",
        "                    namespace=\"company-documents\"\n",
        "                )\n",
        "                print(f\">>> Upserted batch {i//pinecone_batch_size + 1}/{(len(vectors_to_upsert) + pinecone_batch_size - 1)//pinecone_batch_size}\")\n",
        "\n",
        "            print(f\">>> Successfully crawled and stored {total_processed_count} pages ({len(vectors_to_upsert)} chunks)\")\n",
        "            print(f\">>> Processed URLs: {len(all_processed_urls)}/{len(sitemap_urls)}\")\n",
        "\n",
        "            # summary statistics\n",
        "            return {\n",
        "                'success': True,\n",
        "                'pages_processed': total_processed_count,\n",
        "                'chunks_stored': len(vectors_to_upsert),\n",
        "                'total_sitemap_urls': len(sitemap_urls),\n",
        "                'processed_urls': all_processed_urls,\n",
        "                'timestamp': int(time.time())\n",
        "            }\n",
        "        else:\n",
        "            print(\">>>  No content was processed and stored\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': 'No content was processed and stored',\n",
        "                'pages_processed': 0,\n",
        "                'chunks_stored': 0,\n",
        "                'total_sitemap_urls': len(sitemap_urls)\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\">>> Website crawl error: {e}\")\n",
        "        return {\n",
        "            'success': False,\n",
        "            'error': str(e),\n",
        "            'pages_processed': 0,\n",
        "            'chunks_stored': 0\n",
        "        }"
      ],
      "metadata": {
        "id": "oEuD9rkXFvJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone Stored Data Verification"
      ],
      "metadata": {
        "id": "9VqE9vprF-pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_stored_data():\n",
        "    \"\"\"Verify that data was stored correctly in Pinecone\"\"\"\n",
        "    global pinecone_index\n",
        "\n",
        "    try:\n",
        "        print(\">>> Verifying stored data...\")\n",
        "\n",
        "        # Get index stats\n",
        "        stats = pinecone_index.describe_index_stats()\n",
        "        namespace_stats = stats.namespaces.get(\"company-documents\", {})\n",
        "        vector_count = namespace_stats.get('vector_count', 0)\n",
        "\n",
        "        print(f\">>> Total vectors in 'company-documents' namespace: {vector_count}\")\n",
        "\n",
        "        # Sample some data\n",
        "        import random\n",
        "        random_vector = [random.random() for _ in range(384)]\n",
        "\n",
        "        results = pinecone_index.query(\n",
        "            vector=random_vector,\n",
        "            top_k=5,\n",
        "            namespace=\"company-documents\",\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        print(f\">>> Sample of stored data:\")\n",
        "        aven_count = 0\n",
        "\n",
        "        for i, match in enumerate(results.matches):\n",
        "            metadata = match.metadata\n",
        "            source = metadata.get('source', 'Unknown')\n",
        "            title = metadata.get('title', 'Unknown')[:50]\n",
        "            domain = urlparse(source).netloc.lower() if source else 'unknown'\n",
        "            is_aven = domain == 'aven.com' or domain.endswith('.aven.com')\n",
        "\n",
        "            if is_aven:\n",
        "                aven_count += 1\n",
        "\n",
        "            print(f\"  {i+1}. {title} | {domain} | {'>> Aven' if is_aven else '>> Non-Aven'}\")\n",
        "\n",
        "        verification_result = {\n",
        "            'total_vectors': vector_count,\n",
        "            'sample_size': len(results.matches),\n",
        "            'aven_sources': aven_count,\n",
        "            'aven_percentage': (aven_count / len(results.matches) * 100) if results.matches else 0,\n",
        "            'is_properly_filtered': aven_count == len(results.matches)\n",
        "        }\n",
        "\n",
        "        print(f\">>> Verification complete: {aven_count}/{len(results.matches)} sources are from Aven\")\n",
        "        return verification_result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\">>> Verification failed: {e}\")\n",
        "        return {'error': str(e)}"
      ],
      "metadata": {
        "id": "TcylPv51GBAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN - Comprehensive Crawling Execution"
      ],
      "metadata": {
        "id": "-SKRAat3GkBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\">>> Starting comprehensive crawl of Aven website...\")\n",
        "print(\"This may take several minutes depending on the website size.\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Execute the crawl\n",
        "crawl_results = crawl_and_store_website()\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== CRAWL COMPLETED! ===\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if crawl_results['success']:\n",
        "    print(f\">>> Success! Crawled and stored website content.\")\n",
        "    print(f\">>> Pages processed: {crawl_results['pages_processed']}\")\n",
        "    print(f\">>> Chunks stored: {crawl_results['chunks_stored']}\")\n",
        "    print(f\">>> Total sitemap URLs: {crawl_results['total_sitemap_urls']}\")\n",
        "    print(f\">>> Duration: {duration:.1f} seconds\")\n",
        "    print(f\">>> Completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Run verification\n",
        "    print(\"\\n>>> Running verification...\")\n",
        "    verification = verify_stored_data()\n",
        "    if 'error' not in verification:\n",
        "        print(f\">>> Verification Results:\")\n",
        "        print(f\"   Total vectors: {verification['total_vectors']}\")\n",
        "        print(f\"   Aven sources: {verification['aven_percentage']:.1f}%\")\n",
        "        print(f\"   Filtering status: {'>> Success' if verification['is_properly_filtered'] else '>>  Some non-Aven content detected'}\")\n",
        "else:\n",
        "    print(f\">> Crawl failed: {crawl_results.get('error', 'Unknown error')}\")\n",
        ""
      ],
      "metadata": {
        "id": "mWFDk9CeGie_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional"
      ],
      "metadata": {
        "id": "UN348bWoG4LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MANUAL VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "# to manually verify the stored data\n",
        "\"\"\"\n",
        "print(\">>>  Manual verification of stored data...\")\n",
        "verification_results = verify_stored_data()\n",
        "print(\"\\nVerification Results:\")\n",
        "for key, value in verification_results.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# VIEW SITEMAP URLS\n",
        "# =============================================================================\n",
        "\n",
        "# to see what URLs were found in the sitemap\n",
        "\"\"\"\n",
        "print(\">>>  Fetching sitemap URLs for inspection...\")\n",
        "urls = get_sitemap_urls()\n",
        "print(f\"\\nFound {len(urls)} URLs in sitemap:\")\n",
        "for i, url in enumerate(urls[:20], 1):  # Show first 20 URLs\n",
        "    print(f\"  {i}. {url}\")\n",
        "if len(urls) > 20:\n",
        "    print(f\"  ... and {len(urls) - 20} more URLs\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LNI2sWmTBhG4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}